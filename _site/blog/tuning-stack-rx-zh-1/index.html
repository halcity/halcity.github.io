<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
       'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
       })(window,document,'script','dataLayer','GTM-WP54B9K');</script>
    <!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>[译] Linux网络栈监控和调优：接收数据 1</title>

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="ArthurChiao's Blog" href="http://0.0.0.0:4000/feed.xml" />
    
    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/tuning-stack-rx-zh-1/">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>

    
</head>


<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WP54B9K"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/">ArthurChiao's Blog</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                   
                   
                   <li class="element first  ">
                     <a href="/index.html">Home</a>
                   </li> 
                 
                   
                   
                   <li class="element   ">
                     <a href="/articles">Articles</a>
                   </li> 
                 
                   
                   
                   <li class="element   ">
                     <a href="/articles-zh">Articles (中文)</a>
                   </li> 
                 
                   
                   
                   <li class="element   ">
                     <a href="/categories">Categories</a>
                   </li> 
                 
                   
                   
                   <li class="element   last">
                     <a href="/about">About</a>
                   </li> 
                 
                 <!-- <li> <a href="https://github.com/arthurchiao/reading" target="_blank">Reading</a></li> -->
                 <!-- <li> <a href="https://github.com/arthurchiao/" target="_blank">GitHub</a></li> -->
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post">
  
  <h1 class="postTitle">[译] Linux网络栈监控和调优：接收数据 1</h1>
  <p class="meta">2018-12-05 | <span class="time">24</span> Minute Read</p>

  
  
  <h3 id="译者序">译者序</h3>

<p>本文翻译自2016年的一篇英文博客 <a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a>。<strong>如果能看懂英文，我建议你阅读原文，或者和本文对照看。</strong></p>

<p>这篇文章写的是<strong>“Linux networking stack”</strong>，这里的”stack“并不仅仅是内核协议栈，而是包
括内核协议栈在内的、从数据包到达物理网卡到最终被用户态程序收起的整个路径。所以文
章有三方面，交织在一起，看起来非常累（但是很过瘾）：</p>

<ol>
  <li>原理及代码实现：网络各层，包括驱动、硬中断、软中断、内核协议栈、socket等等</li>
  <li>监控：对代码中的重要计数进行监控，一般在<code class="highlighter-rouge">/proc</code>或<code class="highlighter-rouge">/sys</code>下面有对应输出</li>
  <li>调优：修改网络配置参数</li>
</ol>

<p>本文的另一个特色是，几乎所有讨论的内核代码，都在相应的地方给出了github上的链接，
具体到行。</p>

<p>网络栈非常复杂，原文太长又没有任何章节号，所以看起来非常累。因此在翻译的时候，我
将其分为了若干篇，并添加了相应的章节号，以期按图索骥。</p>

<p>以下是翻译。</p>

<hr />

<p>这篇文章的姊妹篇 <a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">Monitoring and Tuning the Linux Networking Stack: Sending Data</a>。</p>

<p>这篇文章的图文注释版 <a href="https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/">the Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data</a>。</p>

<h3 id="写给不想读长文的人tl-dr">写给不想读长文的人（TL; DR）</h3>

<p>本文章介绍了Linux内核是如何<strong>收包</strong>（receive packets）的，包是怎样从网络栈到
用户空间程序的，以及如何<strong>监控</strong>（monitoring）和<strong>调优</strong>（tuning）这一路径上的各个网络栈组件。</p>

<p>想对Linux网络栈进行监控或调优，必须对其正在发生什么有一个深入的理解，
而这离不开读内核源码。希望本文可以给那些正准备投身于此的人提供一份参考。</p>

<h3 id="特别鸣谢">特别鸣谢</h3>

<p>特别感谢<a href="https://privateinternetaccess.com/">Private Internet Access</a>的各位同僚
。公司雇佣我们做包括本文主题在内的网络研究，并非常慷慨地允许我们将研究成果以文章
的形式发表。</p>

<p>这篇文章基于在<a href="https://privateinternetaccess.com/">Private Internet Access</a>的研
究成果，最开始以<a href="https://www.privateinternetaccess.com/blog/2016/01/linux-networking-stack-from-the-ground-up-part-1/">5篇连载
</a>
的形式出现。</p>

<h2 id="1-监控和调优网络栈常规建议">1 监控和调优网络栈：常规建议</h2>

<p>网络栈很复杂，没有一种方式适用于所有场景。如果性能和网络健康状态对你或你的业
务非常重要，那你没有别的选择，只能花大量的时间、精力和金钱去深入理解系统的各
个部分之间是如何交互的。</p>

<p>理想情况下，你应该考虑在网络栈的各层测量丢包状况，这样就可以缩小范围，确定那个组
件需要调优。</p>

<p><strong>然后，这也是一些网络管理员开始走偏的地方</strong>：他们想当然的认为通过一翻<code class="highlighter-rouge">sysctl</code>配
置或查看<code class="highlighter-rouge">/proc</code>等方式可以解决问题，并适用于所有场景。在某些场景下，可能确实能解
决问题，但是，整个系统是如此细微地缠绕交织在一起，如果你想做有意义的监控和调优的
话，你必须得努力在更深层次搞清系统是如何工作的。否则，你虽然可以使用默认配置，并
在相当长的时间内表现良好，但终会到某个时间点，你不得不（投时间、精力和金钱研究这
些配置，然后）做优化。</p>

<p>本文中的一些示例配置仅为了方便理解（效果），并不作为任何特定配置或默认配置的建议
。在做任何配置改动之前，你应该有一个能够对系统进行监控的框架，以查看变更是否带来
预期的效果。</p>

<p>对远程连接上的机器进行网络变更是相当危险的，机器很可能失联。另外，不要在生产环境
直接调整这些配置；如果可能的话，在新机器上改配置，然后将机器灰度上线到生产。</p>

<h2 id="2-收包过程俯瞰">2 收包过程俯瞰</h2>

<p>本文将拿<strong>Intel I350</strong>网卡的<code class="highlighter-rouge">igb</code>驱动作为参考，网卡的data sheet这里可以下载<a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf">PDF</a>（警告：文件很大）。</p>

<p>从比较高的层次看，一个数据包从被网卡接收直到进入socket的接收队列的整个过程如下：</p>

<ol>
  <li>加载网卡驱动，初始化</li>
  <li>包从外部网络进入网卡</li>
  <li>网卡（通过直接内存访问，DMA）将包copy到内核内存中的ring buffer</li>
  <li>产生硬件中断，通知系统收到了一个包</li>
  <li>驱动调用NAPI，如果轮询（poll）还没开始，就开始轮询</li>
  <li><code class="highlighter-rouge">ksoftirqd</code>进程调用NAPI的<code class="highlighter-rouge">poll</code>函数从ring buffer收包（<code class="highlighter-rouge">poll</code>函数是网卡驱动在初始化阶
段注册的；每个CPU上都运行着一个<code class="highlighter-rouge">ksoftirqd</code>进程，在系统启动期间就注册了）</li>
  <li>ring buffer里包对应的内存区域解除映射（unmapped）</li>
  <li>通过DMA进入内存的数据包以<code class="highlighter-rouge">skb</code>的形式被送至更上层处理</li>
  <li>如果packet steering功能打开，或者网卡有多队列，网卡收到的包会被分发到多个CPU</li>
  <li>包从队列进入协议层</li>
  <li>协议层处理包</li>
  <li>包从协议层进入相应socket的接收队列</li>
</ol>

<p>接下来会详细介绍这个过程。</p>

<p>协议层分析我们将会关注IP和UDP层，其他协议层可参考这个过程。</p>

<h2 id="3-网络设备驱动">3 网络设备驱动</h2>

<p>本文基于Linux 3.13。</p>

<p>准确地理解Linux内核的收包过程非常有挑战性。我们需要仔细查看网卡驱动是如何工作的
，才能对网络栈的相应部分有更加清晰的理解。</p>

<p>本文将拿<code class="highlighter-rouge">ibg</code>驱动作为例子，它是常见的Intel I350网卡的驱动。我们先来理解网卡驱动是如何
工作的。</p>

<h3 id="31-初始化">3.1 初始化</h3>

<p>驱动会使用<code class="highlighter-rouge">module_init</code>向内核注册一个初始化函数，当驱动被加载时，内核会调用这个函数。</p>

<p>这个初始化函数(<code class="highlighter-rouge">igb_init_module</code>)的代码见 <a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L676-L697"><code class="highlighter-rouge">drivers/net/ethernet/intel/igb/igb_main.c</code></a>.</p>

<p>过程非常简单直接：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
 *  igb_init_module - Driver Registration Routine
 *
 *  igb_init_module is the first routine called when the driver is
 *  loaded. All it does is register with the PCI subsystem.
 **/</span>
<span class="k">static</span> <span class="kt">int</span> <span class="n">__init</span> <span class="nf">igb_init_module</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">int</span> <span class="n">ret</span><span class="p">;</span>
  <span class="n">pr_info</span><span class="p">(</span><span class="s">"%s - version %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">igb_driver_string</span><span class="p">,</span> <span class="n">igb_driver_version</span><span class="p">);</span>
  <span class="n">pr_info</span><span class="p">(</span><span class="s">"%s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">igb_copyright</span><span class="p">);</span>

  <span class="cm">/* ... */</span>

  <span class="n">ret</span> <span class="o">=</span> <span class="n">pci_register_driver</span><span class="p">(</span><span class="o">&amp;</span><span class="n">igb_driver</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">module_init</span><span class="p">(</span><span class="n">igb_init_module</span><span class="p">);</span>
</code></pre></div></div>

<p>初始化的大部分工作在<code class="highlighter-rouge">pci_register_driver</code>里面完成，接下来细看。</p>

<h4 id="pci初始化">PCI初始化</h4>

<p>Intel I350 网卡是<a href="https://en.wikipedia.org/wiki/PCI_Express">PCI express</a>设备。</p>

<p>PCI设备通过<a href="https://en.wikipedia.org/wiki/PCI_configuration_space#Standardized_registers">PCI Configuration
Space</a>
里面的寄存器识别自己。</p>

<p>当设备驱动编译的时候，宏<code class="highlighter-rouge">MODULE_DEVICE_TABLE</code>(定义在
<a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/module.h#L145-L146"><code class="highlighter-rouge">include/module.h</code></a>)
会导出一个PCI设备ID表，驱动据此识别它可以控制的设备。内核也会依据这个设备表判断
对哪个设备加载哪个驱动。</p>

<p><code class="highlighter-rouge">igb</code>驱动的设备表和PCI设备ID分别见：
<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L79-L117">drivers/net/ethernet/intel/igb/igb_main.c</a>
和<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/e1000_hw.h#L41-L75">drivers/net/ethernet/intel/igb/e1000_hw.h</a>。</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="n">DEFINE_PCI_DEVICE_TABLE</span><span class="p">(</span><span class="n">igb_pci_tbl</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I354_BACKPLANE_1GBPS</span><span class="p">)</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I354_SGMII</span><span class="p">)</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I354_BACKPLANE_2_5GBPS</span><span class="p">)</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I211_COPPER</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I210_COPPER</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I210_FIBER</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I210_SERDES</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I210_SGMII</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I210_COPPER_FLASHLESS</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>
  <span class="p">{</span> <span class="n">PCI_VDEVICE</span><span class="p">(</span><span class="n">INTEL</span><span class="p">,</span> <span class="n">E1000_DEV_ID_I210_SERDES_FLASHLESS</span><span class="p">),</span> <span class="n">board_82575</span> <span class="p">},</span>

  <span class="cm">/* ... */</span>
<span class="p">};</span>
<span class="n">MODULE_DEVICE_TABLE</span><span class="p">(</span><span class="n">pci</span><span class="p">,</span> <span class="n">igb_pci_tbl</span><span class="p">);</span>
</code></pre></div></div>

<p>前面提到，驱动初始化的时候会调用<code class="highlighter-rouge">pci_register_driver</code>，这个函数会将该驱动的各
种回调方法注册到一个<code class="highlighter-rouge">struct pci_driver</code>实例，<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L238-L249">drivers/net/ethernet/intel/igb/igb_main.c</a>：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="k">struct</span> <span class="n">pci_driver</span> <span class="n">igb_driver</span> <span class="o">=</span> <span class="p">{</span>
  <span class="p">.</span><span class="n">name</span>     <span class="o">=</span> <span class="n">igb_driver_name</span><span class="p">,</span>
  <span class="p">.</span><span class="n">id_table</span> <span class="o">=</span> <span class="n">igb_pci_tbl</span><span class="p">,</span>
  <span class="p">.</span><span class="n">probe</span>    <span class="o">=</span> <span class="n">igb_probe</span><span class="p">,</span>
  <span class="p">.</span><span class="n">remove</span>   <span class="o">=</span> <span class="n">igb_remove</span><span class="p">,</span>

  <span class="cm">/* ... */</span>
<span class="p">};</span>
</code></pre></div></div>

<h3 id="32-网络设备初始化">3.2 网络设备初始化</h3>

<p>通过PCI ID识别设备后，内核就会为它选择合适的驱动。每个PCI驱动注册了一个<code class="highlighter-rouge">probe()</code>
方法，内核会对每个设备依次调用驱动的<code class="highlighter-rouge">probe</code>方法，当找到一个合适的驱动
后，就不会再为这个设备尝试其他驱动。</p>

<p>很多驱动都需要大量代码来使得设备ready，具体做的事情各有差异。典型的过程：</p>

<ol>
  <li>启用PCI设备</li>
  <li>请求（requesting）内存范围和IO端口</li>
  <li>设置DMA掩码</li>
  <li>注册设备驱动支持的ethtool方法（后面介绍）</li>
  <li>注册所需的watchdog（例如，e1000e有一个检测设备是否僵死的watchdog）</li>
  <li>其他和具体设备相关的事情，例如一些workaround，或者特定硬件的怪异处理</li>
  <li>创建、初始化和注册一个<code class="highlighter-rouge">struct net_device_ops</code>类型实例，包含了用于设备相关的回
调函数，例如打开设备、发送数据到网络、设置MAC地址等</li>
  <li>创建、初始化和注册一个更高层的<code class="highlighter-rouge">struct net_device</code>类型实例，一个实例就代表了一
个设备</li>
</ol>

<p>我们来简单看下<code class="highlighter-rouge">igb</code>驱动的<code class="highlighter-rouge">igb_probe</code>包含哪些过程。下面的代码来自<code class="highlighter-rouge">igb_probe</code>，<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">drivers/net/ethernet/intel/igb/igb_main.c</a>：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">err</span> <span class="o">=</span> <span class="n">pci_enable_device_mem</span><span class="p">(</span><span class="n">pdev</span><span class="p">);</span>

<span class="cm">/* ... */</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">dma_set_mask_and_coherent</span><span class="p">(</span><span class="o">&amp;</span><span class="n">pdev</span><span class="o">-&gt;</span><span class="n">dev</span><span class="p">,</span> <span class="n">DMA_BIT_MASK</span><span class="p">(</span><span class="mi">64</span><span class="p">));</span>

<span class="cm">/* ... */</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">pci_request_selected_regions</span><span class="p">(</span><span class="n">pdev</span><span class="p">,</span> <span class="n">pci_select_bars</span><span class="p">(</span><span class="n">pdev</span><span class="p">,</span>
           <span class="n">IORESOURCE_MEM</span><span class="p">),</span>
           <span class="n">igb_driver_name</span><span class="p">);</span>

<span class="n">pci_enable_pcie_error_reporting</span><span class="p">(</span><span class="n">pdev</span><span class="p">);</span>

<span class="n">pci_set_master</span><span class="p">(</span><span class="n">pdev</span><span class="p">);</span>
<span class="n">pci_save_state</span><span class="p">(</span><span class="n">pdev</span><span class="p">);</span>
</code></pre></div></div>

<h4 id="更多pci驱动信息">更多PCI驱动信息</h4>

<p>详细的PCI驱动讨论不在本文范围，如果想进一步了解，如下材料很推荐：
<a href="http://free-electrons.com/doc/pci-drivers.pdf">分享</a>，
<a href="http://wiki.osdev.org/PCI">wiki</a>，
<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/PCI/pci.txt">Linux Kernel Documentation: PCI</a>。</p>

<h3 id="33-网络设备启动">3.3 网络设备启动</h3>

<p><code class="highlighter-rouge">igb_probe</code>做了很多重要的设备初始化工作。除了PCI相关的，还有如下一些通用网络功能和网络设备相关的工作：</p>

<ol>
  <li>注册<code class="highlighter-rouge">struct net_device_ops</code>实例</li>
  <li>注册ethtool相关的方法</li>
  <li>从网卡获取默认MAC地址</li>
  <li>设置<code class="highlighter-rouge">net_device</code>特性标记</li>
</ol>

<p>我们逐一看一下这些过程，后面会用到。</p>

<h4 id="331-struct-net_device_ops">3.3.1 <code class="highlighter-rouge">struct net_device_ops</code></h4>

<p>网络设备相关的操作函数都注册到这个类型的实例中。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">igb_main.c</a>：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="k">const</span> <span class="k">struct</span> <span class="n">net_device_ops</span> <span class="n">igb_netdev_ops</span> <span class="o">=</span> <span class="p">{</span>
  <span class="p">.</span><span class="n">ndo_open</span>               <span class="o">=</span> <span class="n">igb_open</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_stop</span>               <span class="o">=</span> <span class="n">igb_close</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_start_xmit</span>         <span class="o">=</span> <span class="n">igb_xmit_frame</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_get_stats64</span>        <span class="o">=</span> <span class="n">igb_get_stats64</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_set_rx_mode</span>        <span class="o">=</span> <span class="n">igb_set_rx_mode</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_set_mac_address</span>    <span class="o">=</span> <span class="n">igb_set_mac</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_change_mtu</span>         <span class="o">=</span> <span class="n">igb_change_mtu</span><span class="p">,</span>
  <span class="p">.</span><span class="n">ndo_do_ioctl</span>           <span class="o">=</span> <span class="n">igb_ioctl</span><span class="p">,</span>

  <span class="cm">/* ... */</span>
</code></pre></div></div>

<p>这个实例会在<code class="highlighter-rouge">igb_probe()</code>中赋给<code class="highlighter-rouge">struct net_device</code>中的<code class="highlighter-rouge">netdev_ops</code>字段：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">int</span> <span class="nf">igb_probe</span><span class="p">(</span><span class="k">struct</span> <span class="n">pci_dev</span> <span class="o">*</span><span class="n">pdev</span><span class="p">,</span> <span class="k">const</span> <span class="k">struct</span> <span class="n">pci_device_id</span> <span class="o">*</span><span class="n">ent</span><span class="p">)</span>
<span class="p">{</span>
  <span class="p">...</span>

  <span class="n">netdev</span><span class="o">-&gt;</span><span class="n">netdev_ops</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">igb_netdev_ops</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="332-ethtool函数注册">3.3.2 ethtool函数注册</h4>

<p><a href="https://www.kernel.org/pub/software/network/ethtool/"><code class="highlighter-rouge">ethtool</code></a>是一个命令行工
具，可以查看和修改网络设备的一些配置，常用于收集网卡统计数据。在Ubuntu上，可以通
过<code class="highlighter-rouge">apt-get install ethtool</code>安装。</p>

<p>ethtool通过
<a href="http://man7.org/linux/man-pages/man2/ioctl.2.html">ioctl</a>
和设备驱动通信。内核实现了一个通用ethtool接口，网卡驱动实现这些接口，就可以被
ethtool调用。当ethtool发起一个系统调用之后，内核会找到对应操作的回
调函数。回调实现了各种简单和复杂的函数，简单的如改变一个flag值，复杂的包括调整网
卡硬件如何运行。</p>

<p>相关实现见：<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_ethtool.c">igb_ethtool.c</a>。</p>

<h4 id="333-软中断irq">3.3.3 软中断（IRQ）</h4>

<p>当一个数据帧通过DMA写到RAM（内存）后，网卡是如何通知其他系统这个包可以被处理了呢
？</p>

<p>传统的方式是，网卡会产生一个硬件中断（IRQ），通知数据包到了。有三种常见的硬中断类
型：</p>

<ul>
  <li>MSI-X</li>
  <li>MSI</li>
  <li>legacy IRQ</li>
</ul>

<p>稍后会介绍到。</p>

<p>如果有大量的数据包到达，会产生大量的硬件中断，CPU忙于处理硬件中断的时候，可用于
处理其他任务的时间就会减少。</p>

<p>NAPI（New API)是一种新的机制，可以减少产生的硬件中断的数量，但不能完全消除硬中断
。</p>

<h4 id="334-napi">3.3.4 NAPI</h4>

<h5 id="napi">NAPI</h5>

<p>NAPI接收数据包的方式和传统方式不同，它允许设备驱动注册一个<code class="highlighter-rouge">poll</code>方法，然后调用这
个方法完成收包。</p>

<p>NAPI的使用方式：</p>

<ol>
  <li>驱动打开了NAPI功能，但默认处于未工作状态（没有在收包）</li>
  <li>数据包到达，网卡通过DMA写到内存</li>
  <li>网卡触发一个硬中断，中断处理函数开始执行</li>
  <li>软中断（softirq，稍后介绍），唤醒NAPI子系统。这会触发在一个单独的线程里，调用
驱动注册的<code class="highlighter-rouge">poll</code>方法收包</li>
  <li>驱动禁止网卡产生新的硬件中断。这样做是为了NAPI能够在收包的时候不会被新的中断
打扰</li>
  <li>一旦没有包需要收了，NAPI关闭，网卡的硬中断重新开启</li>
  <li>转步骤2</li>
</ol>

<p>和传统方式相比，NAPI一次中断会接收多个packet，因此可以减少硬件中断的数量。</p>

<p><code class="highlighter-rouge">poll</code>方法是通过调用<code class="highlighter-rouge">netif_napi_add</code>注册到NAPI的，同时还可以指定权重<code class="highlighter-rouge">weight</code>，大
部分驱动都hardcode为64。后面会进一步解释这个weight以及hardcode 64。</p>

<p>通常来说，驱动在初始化的时候注册NAPI poll方法。</p>

<h4 id="335-igb驱动的napi初始化">3.3.5 <code class="highlighter-rouge">igb</code>驱动的NAPI初始化</h4>

<p><code class="highlighter-rouge">igb</code>驱动的初始化过程是一个很长的调用链：</p>

<ol>
  <li><code class="highlighter-rouge">igb_probe</code> <code class="highlighter-rouge">-&gt;</code> <code class="highlighter-rouge">igb_sw_init</code>.</li>
  <li><code class="highlighter-rouge">igb_sw_init</code> <code class="highlighter-rouge">-&gt;</code> <code class="highlighter-rouge">igb_init_interrupt_scheme</code>.</li>
  <li><code class="highlighter-rouge">igb_init_interrupt_scheme</code> <code class="highlighter-rouge">-&gt;</code> <code class="highlighter-rouge">igb_alloc_q_vectors</code>.</li>
  <li><code class="highlighter-rouge">igb_alloc_q_vectors</code> <code class="highlighter-rouge">-&gt;</code> <code class="highlighter-rouge">igb_alloc_q_vector</code>.</li>
  <li><code class="highlighter-rouge">igb_alloc_q_vector</code> <code class="highlighter-rouge">-&gt;</code> <code class="highlighter-rouge">netif_napi_add</code>.</li>
</ol>

<p>从比较高的层面来看，这个调用过程会做以下事情：</p>

<ol>
  <li>如果支持<code class="highlighter-rouge">MSI-X</code>，调用<code class="highlighter-rouge">pci_enable_msix</code>打开它</li>
  <li>计算和初始化一些配置，包括网卡收发队列的数量</li>
  <li>调用<code class="highlighter-rouge">igb_alloc_q_vector</code>创建每个发送和接收队列</li>
  <li><code class="highlighter-rouge">igb_alloc_q_vector</code>会进一步调用<code class="highlighter-rouge">netif_napi_add</code>注册poll方法到NAPI实例</li>
</ol>

<p>我们来看下<code class="highlighter-rouge">igb_alloc_q_vector</code>是如何注册poll方法和私有数据的：
<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1145-L1271">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">int</span> <span class="nf">igb_alloc_q_vector</span><span class="p">(</span><span class="k">struct</span> <span class="n">igb_adapter</span> <span class="o">*</span><span class="n">adapter</span><span class="p">,</span>
                              <span class="kt">int</span> <span class="n">v_count</span><span class="p">,</span> <span class="kt">int</span> <span class="n">v_idx</span><span class="p">,</span>
                              <span class="kt">int</span> <span class="n">txr_count</span><span class="p">,</span> <span class="kt">int</span> <span class="n">txr_idx</span><span class="p">,</span>
                              <span class="kt">int</span> <span class="n">rxr_count</span><span class="p">,</span> <span class="kt">int</span> <span class="n">rxr_idx</span><span class="p">)</span>
<span class="p">{</span>
  <span class="cm">/* ... */</span>

  <span class="cm">/* allocate q_vector and rings */</span>
  <span class="n">q_vector</span> <span class="o">=</span> <span class="n">kzalloc</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">GFP_KERNEL</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">q_vector</span><span class="p">)</span>
          <span class="k">return</span> <span class="o">-</span><span class="n">ENOMEM</span><span class="p">;</span>

  <span class="cm">/* initialize NAPI */</span>
  <span class="n">netif_napi_add</span><span class="p">(</span><span class="n">adapter</span><span class="o">-&gt;</span><span class="n">netdev</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">q_vector</span><span class="o">-&gt;</span><span class="n">napi</span><span class="p">,</span> <span class="n">igb_poll</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>

  <span class="cm">/* ... */</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">q_vector</code>是新分配的队列，<code class="highlighter-rouge">igb_poll</code>是poll方法，当它收包的时候，会通过
这个接收队列找到关联的NAPI实例（<code class="highlighter-rouge">q_vector-&gt;napi</code>）。</p>

<p>这里很重要，后面我们介绍从驱动到网络协议栈的flow（根据IP头信息做哈希，哈希相同的
属于同一个flow）时会看到。</p>

<h3 id="34-启用网卡-bring-up">3.4 启用网卡 (Bring Up)</h3>

<p>回忆前面我们提到的<code class="highlighter-rouge">structure net_device_ops</code>实例，它包含网卡启用、发包、设置mac
地址等回调函数（函数指针）。</p>

<p>当启用一个网卡时（例如，通过<code class="highlighter-rouge">ifconfig eth0 up</code>），<code class="highlighter-rouge">net_device_ops</code>的<code class="highlighter-rouge">ndo_open</code>方
法会被调用。它通常会做以下事情：</p>

<ol>
  <li>分配RX、TX队列内存</li>
  <li>打开NAPI功能</li>
  <li>注册中断处理函数</li>
  <li>打开（允许）硬中断</li>
  <li>其他</li>
</ol>

<p><code class="highlighter-rouge">igb</code>驱动中，这个方法对应的是<code class="highlighter-rouge">igb_open</code>函数。</p>

<h4 id="341-准备从网络接收数据">3.4.1 准备从网络接收数据</h4>

<p>今天的大部分网卡都使用DMA将数据直接写到内存，接下来操作系统可以直接从里面读取。
实现这一目的所使用的数据结构是buffer ring（环形缓冲区）。</p>

<p>要实现这一功能，设备驱动必须和操作系统合作，预留（reserve）出一段内存来给网卡使用。
预留成功后，网卡知道了这块内存的地址，接下来收到的包就会放到这里，他们接下来会被
操作系统取走。</p>

<p>由于这块内存区域是有限的，如果数据包的速率非常快，单个CPU来不及取走这些包，新来
的包就会被丢弃。这时候，Receive Side Scaling（RSS，接收端扩展）或者多队列（
multiqueue）一类的技术可能就会排上用场。</p>

<p>一些网卡有能力将接收到的包写到多个不同的内存区域，每个区域都是独立的接收队列。这
样操作系统就可以利用多个CPU（硬件层面）并行处理进来的包。只有部分网卡支持这个功
能。</p>

<p>Intel I350网卡支持多队列，我们可以在<code class="highlighter-rouge">igb</code>的驱动里看出来。<code class="highlighter-rouge">igb</code>驱动启用的时候，最
开始做的事情之一就是调用<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2801-L2804"><code class="highlighter-rouge">igb_setup_all_rx_resources</code></a>函数。这个函数会接着对每个RX
队列调用<code class="highlighter-rouge">igb_setup_rx_resources</code>, 里面会管理DMA的内存.</p>

<p>如果你对其原理感兴趣，可以进一步查看<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/DMA-API-HOWTO.txt">Linux kernel’s DMA API
HOWTO</a>
。</p>

<p>RX队列的数量和大小可以通过ethtool进行配置，调整这两个参数会对收包或者丢包产生可见影响。</p>

<p>网卡通过对packet头（例如源地址、目的地址、端口等待）做哈希，以此决定将packet放到
哪个RX队列。只有很少的网卡支持调整哈希算法。如果支持的话，那你可以根据算法将特定
的flow发到特定的队列，甚至可以做到在硬件层面直接将某些包丢弃。</p>

<p>一些网卡支持调整RX队列的权重，你可以有意地将更多的流量发到指定的queue。</p>

<p>后面会介绍如何对这些参数进行调优。</p>

<h4 id="342-enable-napi">3.4.2 Enable NAPI</h4>

<p>前面看到了驱动如何注册NAPI <code class="highlighter-rouge">poll</code>方法，但是，一般直到网卡被启用之后，NAPI才被启用。</p>

<p>启用NAPI很简单，调用<code class="highlighter-rouge">napi_enable</code>函数就行，这个函数会设置NAPI实例（<code class="highlighter-rouge">struct napi_struct</code>）的一个表示是否启用的标志位。
前面说到，NAPI启用后并不是立即开始工作（而是等硬中断触发）。</p>

<p>对于<code class="highlighter-rouge">igb</code>，驱动初始化或者通过ethtool修改queue数量或大小的时候，会启用每个<code class="highlighter-rouge">q_vector</code>的NAPI实例。
<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2833-L2834">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">adapter</span><span class="o">-&gt;</span><span class="n">num_q_vectors</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
  <span class="n">napi_enable</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">adapter</span><span class="o">-&gt;</span><span class="n">q_vector</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">napi</span><span class="p">));</span>
</code></pre></div></div>

<h4 id="343-注册中断处理函数">3.4.3 注册中断处理函数</h4>

<p>启用NAPI之后，下一步就是注册中断处理函数。设备有多种方式触发一个中断：</p>

<ul>
  <li>MSI-X</li>
  <li>MSI</li>
  <li>legacy interrupts</li>
</ul>

<p>设备驱动的实现也因此而异。</p>

<p>驱动必须判断出设备支持哪种中断方式，然后注册相应的中断处理函数，这些函数在中断发
生的时候会被执行。</p>

<p>一些驱动，例如<code class="highlighter-rouge">igb</code>，会试图为每种中断类型注册一个中断处理函数，如果注册失败，就尝
试下一种（没测试过的）类型。</p>

<p>MSI-X中断是比较推荐的方式，尤其是对于支持多队列的网卡。因为每个RX队列有独立的
MSI-X中断，因此可以被不同的CPU处理（通过<code class="highlighter-rouge">irqbalance</code>方式，或者修改
<code class="highlighter-rouge">/proc/irq/IRQ_NUMBER/smp_affinity</code>）。我们后面会看到，处理中断的CPU也是后来处理
这个包的CPU。这样的话，从网卡硬件中断的层面，就可以设置让收到的包被不同的CPU处理
。</p>

<p>如果MSI-X不支持，那MSI相比于传统中断方式仍然有一些优势，驱动仍然会优先考虑它。
这个<a href="https://en.wikipedia.org/wiki/Message_Signaled_Interrupts">wiki</a>介绍了更多
关于MSI和MSI-X的信息。</p>

<p>在<code class="highlighter-rouge">igb</code>驱动中，函数<code class="highlighter-rouge">igb_msix_ring</code>, <code class="highlighter-rouge">igb_intr_msi</code>, <code class="highlighter-rouge">igb_intr</code>分别是MSI-X, MSI, 和传统中断方式的中断处理函数。</p>

<p>这段代码显式了驱动是如何尝试各种中断类型的，
<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1360-L1413">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">int</span> <span class="nf">igb_request_irq</span><span class="p">(</span><span class="k">struct</span> <span class="n">igb_adapter</span> <span class="o">*</span><span class="n">adapter</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">struct</span> <span class="n">net_device</span> <span class="o">*</span><span class="n">netdev</span> <span class="o">=</span> <span class="n">adapter</span><span class="o">-&gt;</span><span class="n">netdev</span><span class="p">;</span>
  <span class="k">struct</span> <span class="n">pci_dev</span> <span class="o">*</span><span class="n">pdev</span> <span class="o">=</span> <span class="n">adapter</span><span class="o">-&gt;</span><span class="n">pdev</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">err</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">adapter</span><span class="o">-&gt;</span><span class="n">msix_entries</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">igb_request_msix</span><span class="p">(</span><span class="n">adapter</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">err</span><span class="p">)</span>
      <span class="k">goto</span> <span class="n">request_done</span><span class="p">;</span>
    <span class="cm">/* fall back to MSI */</span>

    <span class="cm">/* ... */</span>
  <span class="p">}</span>

  <span class="cm">/* ... */</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">adapter</span><span class="o">-&gt;</span><span class="n">flags</span> <span class="o">&amp;</span> <span class="n">IGB_FLAG_HAS_MSI</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">request_irq</span><span class="p">(</span><span class="n">pdev</span><span class="o">-&gt;</span><span class="n">irq</span><span class="p">,</span> <span class="n">igb_intr_msi</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
          <span class="n">netdev</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">adapter</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">err</span><span class="p">)</span>
      <span class="k">goto</span> <span class="n">request_done</span><span class="p">;</span>

    <span class="cm">/* fall back to legacy interrupts */</span>

    <span class="cm">/* ... */</span>
  <span class="p">}</span>

  <span class="n">err</span> <span class="o">=</span> <span class="n">request_irq</span><span class="p">(</span><span class="n">pdev</span><span class="o">-&gt;</span><span class="n">irq</span><span class="p">,</span> <span class="n">igb_intr</span><span class="p">,</span> <span class="n">IRQF_SHARED</span><span class="p">,</span>
        <span class="n">netdev</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">adapter</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">err</span><span class="p">)</span>
    <span class="n">dev_err</span><span class="p">(</span><span class="o">&amp;</span><span class="n">pdev</span><span class="o">-&gt;</span><span class="n">dev</span><span class="p">,</span> <span class="s">"Error %d getting interrupt</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">err</span><span class="p">);</span>

<span class="nl">request_done:</span>
  <span class="k">return</span> <span class="n">err</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>这就是<code class="highlighter-rouge">igb</code>驱动注册中断处理函数的过程，这个函数在一个包到达网卡，触发了一个硬件
中断的时候，就会被执行。</p>

<h4 id="344-enable-interrupts">3.4.4 Enable Interrupts</h4>

<p>到这里，几乎所有的准备工作都就绪了。唯一剩下的就是打开硬中断，等待数据包进来。
打开硬中断的方式因硬件而异，<code class="highlighter-rouge">igb</code>驱动是在<code class="highlighter-rouge">__igb_open</code>里调用辅助函数<code class="highlighter-rouge">igb_irq_enable</code>完成的。</p>

<p>中断通过写寄存器的方式打开：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">void</span> <span class="nf">igb_irq_enable</span><span class="p">(</span><span class="k">struct</span> <span class="n">igb_adapter</span> <span class="o">*</span><span class="n">adapter</span><span class="p">)</span>
<span class="p">{</span>

  <span class="cm">/* ... */</span>

    <span class="n">wr32</span><span class="p">(</span><span class="n">E1000_IMS</span><span class="p">,</span> <span class="n">IMS_ENABLE_MASK</span> <span class="o">|</span> <span class="n">E1000_IMS_DRSTA</span><span class="p">);</span>
    <span class="n">wr32</span><span class="p">(</span><span class="n">E1000_IAM</span><span class="p">,</span> <span class="n">IMS_ENABLE_MASK</span> <span class="o">|</span> <span class="n">E1000_IMS_DRSTA</span><span class="p">);</span>

  <span class="cm">/* ... */</span>
<span class="p">}</span>
</code></pre></div></div>

<p>现在，网卡已经启用了。驱动可能还会做一些额外的事情，例如启动定时器，工作队列（
work queue），或者其他硬件相关的设置。这些工作做完后，网卡就可以收包了。</p>

<p>我们接下来看一下监控和调优网卡。</p>

<h3 id="35-网卡监控">3.5 网卡监控</h3>

<p>监控网络设备有几种不同的方式，每种方式的监控粒度（granularity）和复杂度不同。我
们先从最粗的粒度开始，逐步细化。</p>

<h4 id="351-using-ethtool--s">3.5.1 Using ethtool -S</h4>

<p><code class="highlighter-rouge">ethtool -S</code>可以查看网卡统计信息（例如丢弃的包数量）：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-S</span> eth0
NIC statistics:
     rx_packets: 597028087
     tx_packets: 5924278060
     rx_bytes: 112643393747
     tx_bytes: 990080156714
     rx_broadcast: 96
     tx_broadcast: 116
     rx_multicast: 20294528
     ....
</code></pre></div></div>

<p>监控这些数据比较困难。因为用命令行获取很容易，但是以上字段并没有一个统一的标准。
不同的驱动，甚至同一驱动的不同版本可能字段都会有差异。</p>

<p>你可以先粗略的查看“drop”, “buffer”, “miss”等字样。然后，在驱动的源码里找到对应的
更新这些字段的地方，这可能是在软件层面更新的，也有可能是在硬件层面通过寄存器更新
的。如果是通过硬件寄存器的方式，你就得查看网卡的data sheet（说明书），搞清楚这个
寄存器代表什么。ethtoool给出的这些字段名，有一些是有误导性的（misleading）。</p>

<h4 id="352-using-sysfs">3.5.2 Using sysfs</h4>

<p>sysfs也提供了统计信息，但相比于网卡层的统计，要更上层一些。</p>

<p>例如，获取eth0的接收端drop的数量：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /sys/class/net/eth0/statistics/rx_dropped
2
</code></pre></div></div>

<p>不同类型的统计分别位于<code class="highlighter-rouge">/sys/class/net/&lt;NIC&gt;/statistics/</code>下面的不同文件，包括
<code class="highlighter-rouge">collisions</code>, <code class="highlighter-rouge">rx_dropped</code>, <code class="highlighter-rouge">rx_errors</code>, <code class="highlighter-rouge">rx_missed_errors</code>等等。</p>

<p>不幸的是，每种类型代表什么意思，是有驱动来决定的，因此也是有驱动决定何时以及在哪
里更新这些计数的。你可能会发现一些驱动将一些特定类型的错误归类为dro，而另外
一些驱动可能将它们归类为miss。</p>

<p>这些值至关重要，因此你需要查看对应的网卡驱动，搞清楚它们真正代表什么。</p>

<h4 id="353-using-procnetdev">3.5.3 Using <code class="highlighter-rouge">/proc/net/dev</code></h4>

<p><code class="highlighter-rouge">/proc/net/dev</code>提供了更高一层的网卡统计。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /proc/net/dev
Inter-|   Receive                                                |  Transmit
 face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed
  eth0: 110346752214 597737500    0    2    0     0          0  20963860 990024805984 6066582604    0    0    0     0       0          0
    lo: 428349463836 1579868535    0    0    0     0          0         0 428349463836 1579868535    0    0    0     0       0          0
</code></pre></div></div>

<p>这个文件里显式的统计只是sysfs里面的一个子集，但适合作为一个常规的统计参考。</p>

<p>前面的警告（caveat）也适用于此：如果这些数据对你非常重要，那你必须得查看内核源码
、驱动源码和驱动手册，搞清楚每个字段真正代表什么意思，计数是如何以及何时被更新的。</p>

<h3 id="36-网卡调优">3.6 网卡调优</h3>

<h4 id="361-查看rx队列数量">3.6.1 查看RX队列数量</h4>

<p>如果你的网卡及其驱动支持RSS/多队列，那你可能会调整RX queue（也叫RX channel）的数量。
这可以用ethtool完成。</p>

<p>查看RX queue数量：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-l</span> eth0
Channel parameters <span class="k">for </span>eth0:
Pre-set maximums:
RX:   0
TX:   0
Other:    0
Combined: 8
Current hardware settings:
RX:   0
TX:   0
Other:    0
Combined: 4
</code></pre></div></div>

<p>这里可以看到允许的最大值（网卡及驱动限制），以及当前设置的值。</p>

<p>注意：不是所有网卡驱动都支持这个操作。如果你的网卡不支持，会看到如下类似的错误：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-l</span> eth0
Channel parameters <span class="k">for </span>eth0:
Cannot get device channel parameters
: Operation not supported
</code></pre></div></div>

<p>这意味着你的驱动没有实现ethtool的<code class="highlighter-rouge">get_channels</code>方法。可能的原因包括：该网卡不支
持调整RX queue数量，不支持RSS/multiqueue，或者驱动没有更新来支持此功能。</p>

<h4 id="362-调整rx-queues">3.6.2 调整RX queues</h4>

<p><code class="highlighter-rouge">ethtool -L</code>可以修改RX queue数量。</p>

<p>注意：一些网卡和驱动只支持combined queue，这种模式下，RX queue和TX queue是一对一
绑定的，上面的例子我们看到的就是这种。</p>

<p>设置combined类型网卡的收发队列为8个：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-L</span> eth0 combined 8
</code></pre></div></div>

<p>如果你的网卡支持独立的RX和TX队列数量，那你可以只修改RX queue数量：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-L</span> eth0 rx 8
</code></pre></div></div>

<p>注意：对于大部分驱动，修改以上配置会使网卡先down再up，因此会造成丢包。请酌情使用
。</p>

<h3 id="363-调整rx-queue的大小">3.6.3 调整RX queue的大小</h3>

<p>一些网卡和驱动也支持修改RX queue的大小。底层是如何工作的，随硬件而异，但幸运的是
，ethtool提供了一个通用的接口来做这件事情。增加RX queue的大小可以在流量很大的时
候缓解丢包问题，但是，只调整这个还不够，软件层面仍然可能会丢包，因此还需要其他的
一些调优才能彻底的缓解或解决丢包问题。</p>

<p><code class="highlighter-rouge">ethtool -g</code>可以查看queue的大小。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-g</span> eth0
Ring parameters <span class="k">for </span>eth0:
Pre-set maximums:
RX:   4096
RX Mini:  0
RX Jumbo: 0
TX:   4096
Current hardware settings:
RX:   512
RX Mini:  0
RX Jumbo: 0
TX:   512
</code></pre></div></div>

<p>以上显式网卡支持最多4096个接收和发送descriptor（描述符，简单理解，存放的是指向包
的指针），但是现在只用到了512个。</p>

<p>用<code class="highlighter-rouge">ethtool -G</code>修改queue大小：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-G</span> eth0 rx 4096
</code></pre></div></div>

<p>注意：对于大部分驱动，修改以上配置会使网卡先down再up，因此会造成丢包。请酌情使用
。</p>

<h4 id="364-调整rx-queue的权重weight">3.6.4 调整RX queue的权重（weight）</h4>

<p>一些网卡支持给不同的queue设置不同的权重，以此分发不同数量的网卡包到不同的队列。</p>

<p>如果你的网卡支持以下功能，那你可以使用：</p>

<ol>
  <li>网卡支持flow indirection（flow重定向，flow是什么前面提到过）</li>
  <li>网卡驱动实现了<code class="highlighter-rouge">get_rxfh_indir_size</code>和<code class="highlighter-rouge">get_rxfh_indir</code>方法</li>
  <li>使用的ethtool版本足够新，支持<code class="highlighter-rouge">-x</code>和<code class="highlighter-rouge">-X</code>参数来设置indirection table</li>
</ol>

<p><code class="highlighter-rouge">ethtool -x</code>检查flow indirection设置：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-x</span> eth0
RX flow <span class="nb">hash </span>indirection table <span class="k">for </span>eth3 with 2 RX ring<span class="o">(</span>s<span class="o">)</span>:
0: 0 1 0 1 0 1 0 1
8: 0 1 0 1 0 1 0 1
16: 0 1 0 1 0 1 0 1
24: 0 1 0 1 0 1 0 1
</code></pre></div></div>

<p>第一列是哈希值索引，是该行的第一个哈希值；冒号后面的是每个哈希值对于的queue，例
如，第一行分别是哈希值0，1，2，3，4，5，6，7，对应的packet应该分别被放到RX queue
0，1，0，1，0，1，0，1。</p>

<p>例子：在前两个RX queue之间均匀的分发（接收到的包）：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-X</span> eth0 equal 2
</code></pre></div></div>

<p>例子：用<code class="highlighter-rouge">ethtool -X</code>设置自定义权重：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-X</span> eth0 weight 6 2
</code></pre></div></div>

<p>以上命令分别给rx queue 0和rx queue 1不同的权重：6和2，因此queue 0接收到的数量更
多。注意queue一般是和CPU绑定的，因此这也意味着相应的CPU也会花更多的时间片在收包
上。</p>

<p>一些网卡还支持修改计算hash时使用哪些字段。</p>

<h4 id="365-调整rxa-hash-fields-for-network-flows">3.6.5 调整RXA hash fields for network flows</h4>

<p>可以用ethtool调整RSS计算哈希时所使用的字段。</p>

<p>例子：查看UDP RX flow哈希所使用的字段：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-n</span> eth0 rx-flow-hash udp4
UDP over IPV4 flows use these fields <span class="k">for </span>computing Hash flow key:
IP SA
IP DA
</code></pre></div></div>

<p>可以看到只用到了源IP（SA：Source Address）和目的IP。</p>

<p>我们接下来修改一下，加入源端口和目的端口：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-N</span> eth0 rx-flow-hash udp4 sdfn
</code></pre></div></div>

<p><code class="highlighter-rouge">sdfn</code>的具体含义解释起来有点麻烦，请查看ethtool的帮助（man page）。</p>

<p>调整hash所用字段是有用的，而<code class="highlighter-rouge">ntuple</code>过滤对于更加细粒度的flow control更加有用。</p>

<h4 id="366-ntuple-filtering-for-steering-network-flows">3.6.6 ntuple filtering for steering network flows</h4>

<p>一些网卡支持“ntuple filtering”特性。该特性允许用户（通过ethtool）指定一些参数来
在硬件上过滤收到的包，然后将其直接放到特定的RX queue。例如，用户可以指定到特定目
端口的TCP包放到RX queue 1。</p>

<p>Intel的网卡上这个特性叫Intel Ethernet Flow Director，其他厂商可能也有他们的名字
，这些都是出于市场宣传原因，底层原理是类似的。</p>

<p>我们后面会看到，ntuple filtering是一个叫Accelerated Receive Flow Steering (aRFS)功能的
核心部分之一，后者使得ntuple filtering的使用更加方便。</p>

<p>这个特性适用的场景：最大化数据本地性（data locality），以增加CPU处理网络数据时的
缓存命中率。例如，考虑运行在80口的web服务器：</p>

<ol>
  <li>webserver进程运行在80口，并绑定到CPU 2</li>
  <li>和某个RX queue关联的硬中断绑定到CPU 2</li>
  <li>目的端口是80的TCP流量通过ntuple filtering绑定到CPU 2</li>
  <li>接下来所有到80口的流量，从数据包进来到数据到达用户程序的整个过程，都由CPU 2处
理</li>
  <li>仔细监控系统的缓存命中率、网络栈的延迟等信息，以验证以上配置是否生效</li>
</ol>

<p>检查ntuple filtering特性是否打开：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-k</span> eth0
Offload parameters <span class="k">for </span>eth0:
...
ntuple-filters: off
receive-hashing: on
</code></pre></div></div>

<p>可以看到，上面的ntuple是关闭的。</p>

<p>打开：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-K</span> eth0 ntuple on
</code></pre></div></div>

<p>打开ntuple filtering功能，并确认打开之后，可以用<code class="highlighter-rouge">ethtool -u</code>查看当前的ntuple
rules：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-u</span> eth0
40 RX rings available
Total 0 rules
</code></pre></div></div>

<p>可以看到当前没有rules。</p>

<p>我们来加一条：目的端口是80的放到RX queue 2：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-U</span> eth0 flow-type tcp4 dst-port 80 action 2
</code></pre></div></div>

<p>你也可以用ntuple filtering在硬件层面直接drop某些flow的包。当特定IP过来的流量太大
时，这种功能可能会派上用场。更多关于ntuple的信息，参考ethtool man page。</p>

<p><code class="highlighter-rouge">ethtool -S &lt;DEVICE&gt;</code>的输出统计里，Intel的网卡有<code class="highlighter-rouge">fdir_match</code>和<code class="highlighter-rouge">fdir_miss</code>两项，
是和ntuple filtering相关的。关于具体的、详细的统计计数，需要查看相应网卡的设备驱
动和data sheet。</p>


  <!-- POST NAVIGATION -->
  <div class="postNav clearfix">
     
      <a class="prev" href="/blog/ebpf-turn-syscall-to-event-zh/"><span>&laquo;&nbsp;[译] eBPF内核探测：如何将任意系统调用转换成事件</span>
      
    </a>
      
      
      <a class="next" href="/blog/tuning-stack-rx-zh-2/"><span>[译] Linux网络栈监控和调优：接收数据 2&nbsp;&raquo;</span>
       
      </a>
     
  </div>
</div>


         

      </div>
   </div><!-- end .content -->

   <div class="footer">
   <div class="container">
      <p class="copy">&copy; 2016-2019
      <a href="https://arthurchiao.github.io">Arthur Chiao</a>, Powered by
      <a href="http://jekyllrb.com">Jekyll </a>, Theme originated from
      <a href="https://github.com/brianmaierjr/long-haul"> Long Haul.</a>

      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_site_pv"> Site visits:
          <span id="busuanzi_value_site_pv"></span>, powered by<a href="http://busuanzi.ibruce.info/"> busuanzi</a>
      </span>

      </p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/profile.php?id=100014334455077">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://linkedin.com/in/yanan-zhao-7691317b?trk=hp-identity-photo">
                  <svg id="linkedin" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/ArthurChiao">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="mailto:arthurchiao@hotmail.com">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->

   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>


</body>

</html>

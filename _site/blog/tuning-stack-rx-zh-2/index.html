<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
       new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
       j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
       'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
       })(window,document,'script','dataLayer','GTM-WP54B9K');</script>
    <!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>[译] Linux网络栈监控和调优：接收数据 2</title>

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="ArthurChiao's Blog" href="http://0.0.0.0:4000/feed.xml" />
    
    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="http://0.0.0.0:4000/blog/tuning-stack-rx-zh-2/">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>

    
</head>


<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WP54B9K"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/">ArthurChiao's Blog</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                   
                   
                   <li class="element first  ">
                     <a href="/index.html">Home</a>
                   </li> 
                 
                   
                   
                   <li class="element   ">
                     <a href="/articles">Articles</a>
                   </li> 
                 
                   
                   
                   <li class="element   ">
                     <a href="/articles-zh">Articles (中文)</a>
                   </li> 
                 
                   
                   
                   <li class="element   ">
                     <a href="/categories">Categories</a>
                   </li> 
                 
                   
                   
                   <li class="element   last">
                     <a href="/about">About</a>
                   </li> 
                 
                 <!-- <li> <a href="https://github.com/arthurchiao/reading" target="_blank">Reading</a></li> -->
                 <!-- <li> <a href="https://github.com/arthurchiao/" target="_blank">GitHub</a></li> -->
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post">
  
  <h1 class="postTitle">[译] Linux网络栈监控和调优：接收数据 2</h1>
  <p class="meta">2018-12-05 | <span class="time">30</span> Minute Read</p>

  
  
  <h2 id="4-软中断softirq">4 软中断（SoftIRQ）</h2>

<p>在查看网络栈之前，让我们先开个小差，看下内核里一个叫SoftIRQ的东西。</p>

<h3 id="41-软中断是什么">4.1 软中断是什么</h3>

<p>内核的软中断系统是一种在硬中断处理上下文（驱动中）之外执行代码机制。硬件中断处理
函数执行的时候，会屏蔽部分或全部的（新的）硬中断。中断被屏蔽的时间越长，丢失事件
的可能性也就越大。所以，所有耗时的操作都应该从硬中断处理逻辑中剥离出来，硬中断因
此能尽可能快的执行，然后再重新打开硬中断。</p>

<p>内核中也有其他机制将耗时操作转移出去，不过对于网络栈，我们接下来只看软中断。</p>

<p>可以把软中断系统想象成一系列内核线程（每个CPU一个），这些线程执行针对不同事件注
册好的处理函数（handler）。如果你执行过top命令，可能会注意到ksoftirqd/0这个内核
线程，其表示这个软中断线程跑在CPU 0上面。</p>

<p>内核子系统（比如网络）能通过open_softirq函数注册软中断处理函数。接下来我们会看到
网络系统是如何注册它的处理函数的。现在，我们先来学习一下软中断是如何工作的。</p>

<h3 id="42-ksoftirqd">4.2 <code class="highlighter-rouge">ksoftirqd</code></h3>

<p>软中断对分担硬中断的工作如此重要，因此软中断线程在内核启动的很早阶段就spawn出来了。</p>

<p><a href="https://github.com/torvalds/linux/blob/v3.13/kernel/softirq.c#L743-L758"><code class="highlighter-rouge">kernel/softirq.c</code></a> 展示了ksoftirqd系统是如何初始化的：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="k">struct</span> <span class="n">smp_hotplug_thread</span> <span class="n">softirq_threads</span> <span class="o">=</span> <span class="p">{</span>
      <span class="p">.</span><span class="n">store</span>              <span class="o">=</span> <span class="o">&amp;</span><span class="n">ksoftirqd</span><span class="p">,</span>
      <span class="p">.</span><span class="n">thread_should_run</span>  <span class="o">=</span> <span class="n">ksoftirqd_should_run</span><span class="p">,</span>
      <span class="p">.</span><span class="n">thread_fn</span>          <span class="o">=</span> <span class="n">run_ksoftirqd</span><span class="p">,</span>
      <span class="p">.</span><span class="n">thread_comm</span>        <span class="o">=</span> <span class="s">"ksoftirqd/%u"</span><span class="p">,</span>
<span class="p">};</span>

<span class="k">static</span> <span class="n">__init</span> <span class="kt">int</span> <span class="nf">spawn_ksoftirqd</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
      <span class="n">register_cpu_notifier</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cpu_nfb</span><span class="p">);</span>

      <span class="n">BUG_ON</span><span class="p">(</span><span class="n">smpboot_register_percpu_thread</span><span class="p">(</span><span class="o">&amp;</span><span class="n">softirq_threads</span><span class="p">));</span>

      <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">early_initcall</span><span class="p">(</span><span class="n">spawn_ksoftirqd</span><span class="p">);</span>
</code></pre></div></div>

<p>看到注册了两个回调函数: <code class="highlighter-rouge">ksoftirqd_should_run</code>和<code class="highlighter-rouge">run_ksoftirqd</code>。这两个函数都会从
<a href="https://github.com/torvalds/linux/blob/v3.13/kernel/smpboot.c#L94-L163"><code class="highlighter-rouge">kernel/smpboot.c</code></a>
里调用，作为事件处理循环的一部分。</p>

<p><code class="highlighter-rouge">kernel/smpboot.c</code>里面的代码首先调用<code class="highlighter-rouge">ksoftirqd_should_run</code>判断是否有pending的软
中断，如果有，就执行<code class="highlighter-rouge">run_ksoftirqd</code>，后者做一些bookeeping工作，然后调用
<code class="highlighter-rouge">__do_softirq</code>。</p>

<h3 id="43-__do_softirq">4.3 <code class="highlighter-rouge">__do_softirq</code></h3>

<p><code class="highlighter-rouge">__do_softirq</code>做的几件事情：</p>

<ul>
  <li>判断哪个softirq被pending</li>
  <li>计算softirq时间，用于统计</li>
  <li>更新softirq执行相关的统计数据</li>
  <li>执行pending softirq的处理函数</li>
</ul>

<p>查看CPU利用率时，<code class="highlighter-rouge">si</code>字段对应的就是softirq，度量（从硬中断转移过来的）软中断的CPU使用量。</p>

<h3 id="44-监控">4.4 监控</h3>

<p>软中断的信息可以从 <code class="highlighter-rouge">/proc/softirqs</code>读取：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /proc/softirqs
                    CPU0       CPU1       CPU2       CPU3
          HI:          0          0          0          0
       TIMER: 2831512516 1337085411 1103326083 1423923272
      NET_TX:   15774435     779806     733217     749512
      NET_RX: 1671622615 1257853535 2088429526 2674732223
       BLOCK: 1800253852    1466177    1791366     634534
BLOCK_IOPOLL:          0          0          0          0
     TASKLET:         25          0          0          0
       SCHED: 2642378225 1711756029  629040543  682215771
     HRTIMER:    2547911    2046898    1558136    1521176
         RCU: 2056528783 4231862865 3545088730  844379888
</code></pre></div></div>

<p>监控这些数据可以得到软中断的执行频率信息。</p>

<p>例如，<code class="highlighter-rouge">NET_RX</code>一行显示的是软中断在CPU间的分布。如果分布非常不均匀，那某一列的值
就会远大于其他列，这预示着下面要介绍的Receive Packet Steering / Receive Flow
Steering可能会派上用场。在监控这个数据的时候也要注意：不要太相信这个数值，NET_RX
太高并不一定都是网卡触发的，下面会介绍到，其他地方也有可能触发之。</p>

<p>当调整其他网络配置时，也留意下这个指标的变动。</p>

<p>现在，让我们进入网络栈部分，跟踪一个包是如何被接收的。</p>

<h2 id="5-linux网络设备子系统">5 Linux网络设备子系统</h2>

<p>我们已经知道了网络驱动和软中断是如何工作的，接下来看Linux网络设备子系统是如何初始化的。
然后我们就可以从一个包到达网卡开始跟踪它的整个路径。</p>

<h3 id="51-网络设备子系统的初始化">5.1 网络设备子系统的初始化</h3>

<p>网络设备(netdev)的初始化在<code class="highlighter-rouge">net_dev_init</code>，里面有些东西很有意思。</p>

<h4 id="struct-softnet_data实例初始化"><code class="highlighter-rouge">struct softnet_data</code>实例初始化</h4>

<p><code class="highlighter-rouge">net_dev_init</code>为每个CPU创建一个<code class="highlighter-rouge">struct softnet_data</code>实例。这些实例包含一些重要指
针，指向处理网络数据的相关一些信息：</p>

<ul>
  <li>需要注册到这个CPU的NAPI实例列表</li>
  <li>数据处理backlog</li>
  <li>处理权重</li>
  <li>receive offload实例列表</li>
  <li>receive packet steering设置</li>
</ul>

<p>接下来随着逐步进入网络栈，我们会一一查看这些功能。</p>

<h4 id="softirq-handler初始化">SoftIRQ Handler初始化</h4>

<p><code class="highlighter-rouge">net_dev_init</code>分别为接收和发送数据注册了一个软中断处理函数。</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="kt">int</span> <span class="n">__init</span> <span class="nf">net_dev_init</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
  <span class="cm">/* ... */</span>

  <span class="n">open_softirq</span><span class="p">(</span><span class="n">NET_TX_SOFTIRQ</span><span class="p">,</span> <span class="n">net_tx_action</span><span class="p">);</span>
  <span class="n">open_softirq</span><span class="p">(</span><span class="n">NET_RX_SOFTIRQ</span><span class="p">,</span> <span class="n">net_rx_action</span><span class="p">);</span>

 <span class="cm">/* ... */</span>
<span class="p">}</span>
</code></pre></div></div>

<p>后面会看到驱动的中断处理函数是如何触发<code class="highlighter-rouge">net_rx_action</code>这个为<code class="highlighter-rouge">NET_RX_SOFTIRQ</code>软中断注册的处理函数的。</p>

<h3 id="52-数据来了">5.2 数据来了</h3>

<p>终于，网络数据来了！</p>

<p>如果RX队列有足够的描述符（descriptors），包会通过DMA写到RAM。设备然后发起对应于
它的中断（或者在MSI-X的场景，中断和包达到的RX队列绑定）。</p>

<h4 id="521-中断处理函数">5.2.1 中断处理函数</h4>

<p>一般来说，中断处理函数应该将尽可能多的处理逻辑移出（到软中断），这至关重要，因为
发起一个中断后，其他的中断就会被屏蔽。</p>

<p>我们来看一下MSI-X中断处理函数的代码，它展示了中断处理函数是如何尽量简单的。</p>

<p><a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059">igb_main.c</a>：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">static</span> <span class="n">irqreturn_t</span> <span class="nf">igb_msix_ring</span><span class="p">(</span><span class="kt">int</span> <span class="n">irq</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">data</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">struct</span> <span class="n">igb_q_vector</span> <span class="o">*</span><span class="n">q_vector</span> <span class="o">=</span> <span class="n">data</span><span class="p">;</span>

  <span class="cm">/* Write the ITR value calculated from the previous interrupt. */</span>
  <span class="n">igb_write_itr</span><span class="p">(</span><span class="n">q_vector</span><span class="p">);</span>

  <span class="n">napi_schedule</span><span class="p">(</span><span class="o">&amp;</span><span class="n">q_vector</span><span class="o">-&gt;</span><span class="n">napi</span><span class="p">);</span>

  <span class="k">return</span> <span class="n">IRQ_HANDLED</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>这个中断处理函数非常简短，只做了2个很快的操作，然后就返回了。</p>

<p>首先，它调用<code class="highlighter-rouge">igb_write_itr</code> 更新一个硬件寄存器。对这个例子，这个寄存器是记录硬件
中断频率的。</p>

<p>这个寄存器和一个叫”Interrupt Throttling”（也叫”Interrupt Coalescing”）的硬件特性
相关，这个特性可以平滑传送到CPU的中断数量。我们接下来会看到，ethtool是怎么样提供
了一个机制用于调整IRQ触发频率的。</p>

<p>第二，napi_schedule 触发，如果NAPI的处理循环还没开始的话，这会唤醒它。注意，这个
处理循环是在软中断中执行的，而不是硬中断。</p>

<p>这段代码展示了硬中断尽量简短为何如此重要；为我们接下来理解多核CPU的接收逻辑很有
帮助。</p>

<h4 id="522-napi-和-napi_schedule">5.2.2 NAPI 和 <code class="highlighter-rouge">napi_schedule</code></h4>

<p>接下来看从硬件中断中调用的<code class="highlighter-rouge">napi_schedule</code>是如何工作的。</p>

<p>注意，NAPI存在的意义是无需硬件中断通知可以收包了，就可以接收网络数据。前面提到，
NAPI的轮询循环（poll loop）是受硬件中断触发而跑起来的。换句话说，NAPI功能启用了
，但是默认是没有工作的，直到第一个包到达的时候，网卡触发的一个硬件将它唤醒。后面
会看到，也还有其他的情况，NAPI功能也会被关闭，直到下一个硬中断再次将它唤起。</p>

<p><code class="highlighter-rouge">napi_schedule</code>只是一个简单的封装，内层调用<code class="highlighter-rouge">__napi_schedule</code>。
<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168">net/core/dev.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
 * __napi_schedule - schedule for receive
 * @n: entry to schedule
 *
 * The entry's receive function will be scheduled to run
 */</span>
<span class="kt">void</span> <span class="nf">__napi_schedule</span><span class="p">(</span><span class="k">struct</span> <span class="n">napi_struct</span> <span class="o">*</span><span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">flags</span><span class="p">;</span>

  <span class="n">local_irq_save</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
  <span class="n">____napi_schedule</span><span class="p">(</span><span class="o">&amp;</span><span class="n">__get_cpu_var</span><span class="p">(</span><span class="n">softnet_data</span><span class="p">),</span> <span class="n">n</span><span class="p">);</span>
  <span class="n">local_irq_restore</span><span class="p">(</span><span class="n">flags</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">EXPORT_SYMBOL</span><span class="p">(</span><span class="n">__napi_schedule</span><span class="p">);</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">__get_cpu_var</code>用于获取属于这个CPU的<code class="highlighter-rouge">structure softnet_data</code>实例。</p>

<p><code class="highlighter-rouge">____napi_schedule</code>, <a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168">net/core/dev.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Called with irq disabled */</span>
<span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="nf">____napi_schedule</span><span class="p">(</span><span class="k">struct</span> <span class="n">softnet_data</span> <span class="o">*</span><span class="n">sd</span><span class="p">,</span>
                                     <span class="k">struct</span> <span class="n">napi_struct</span> <span class="o">*</span><span class="n">napi</span><span class="p">)</span>
<span class="p">{</span>
  <span class="n">list_add_tail</span><span class="p">(</span><span class="o">&amp;</span><span class="n">napi</span><span class="o">-&gt;</span><span class="n">poll_list</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sd</span><span class="o">-&gt;</span><span class="n">poll_list</span><span class="p">);</span>
  <span class="n">__raise_softirq_irqoff</span><span class="p">(</span><span class="n">NET_RX_SOFTIRQ</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>这段代码了做了两个重要的事情：</p>

<ol>
  <li>将（从驱动的中断函数中传来的）<code class="highlighter-rouge">napi_struct</code>实例，添加到poll list，后者attach到这个CPU上的<code class="highlighter-rouge">softnet_data</code></li>
  <li><code class="highlighter-rouge">__raise_softirq_irqoff</code>触发一个<code class="highlighter-rouge">NET_RX_SOFTIRQ</code>类型软中断。这会触发执行
<code class="highlighter-rouge">net_rx_action</code>（如果没有正在执行），后者是网络设备初始化的时候注册的</li>
</ol>

<p>接下来会看到，软中断处理函数<code class="highlighter-rouge">net_rx_action</code>会调用NAPI的poll函数来收包。</p>

<h4 id="523-关于cpu和网络数据处理的一点笔记">5.2.3 关于CPU和网络数据处理的一点笔记</h4>

<p>注意到目前为止，我们从硬中断处理函数中转移到软中断处理函数的逻辑，都是使用的本
CPU实例。</p>

<p>驱动的硬中断处理函数做的事情很少，但软中断将会在和硬中断相同的CPU上执行。这就是
为什么给每个CPU一个特定的硬中断非常重要：这个CPU不仅处理这个硬中断，而且通过NAPI
处理接下来的软中断来收包。</p>

<p>后面我们会看到，Receive Packet Steering可以将软中断分给其他CPU。</p>

<h4 id="524-监控网络数据到达">5.2.4 监控网络数据到达</h4>

<h5 id="硬中断请求">硬中断请求</h5>

<p>注意：监控硬件中断拿不到关于网络包处理的健康状况的全景图，一些驱动在NAPI运行的
时候会关闭硬中断。这只是你整个监控方案的一个重要部分。</p>

<p>读取硬中断统计：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /proc/interrupts
            CPU0       CPU1       CPU2       CPU3
   0:         46          0          0          0 IR-IO-APIC-edge      timer
   1:          3          0          0          0 IR-IO-APIC-edge      i8042
  30: 3361234770          0          0          0 IR-IO-APIC-fasteoi   aacraid
  64:          0          0          0          0 DMAR_MSI-edge      dmar0
  65:          1          0          0          0 IR-PCI-MSI-edge      eth0
  66:  863649703          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-0
  67:  986285573          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-1
  68:         45          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-2
  69:        394          0          0          0 IR-PCI-MSI-edge      eth0-TxRx-3
 NMI:    9729927    4008190    3068645    3375402  Non-maskable interrupts
 LOC: 2913290785 1585321306 1495872829 1803524526  Local timer interrupts
</code></pre></div></div>

<p>可以看到有多少包进来、硬件中断频率，RX队列被哪个CPU处理等信息。这里只能看到硬中
断数量，不能看出实际多少数据被接收或处理，因为一些驱动在NAPI收包时会关闭硬中断。
进一步，使用Interrupt Coalescing时也会影响这个统计。监控这个指标能帮你判断出你设
置的Interrupt Coalescing是不是在工作。</p>

<p>为了使监控更加完整，需要同时监控<code class="highlighter-rouge">/proc/softirqs</code> (前面提到)和<code class="highlighter-rouge">/proc</code>。</p>

<h4 id="525-数据接收调优">5.2.5 数据接收调优</h4>

<h5 id="中断合并interrupt-coalescing">中断合并（Interrupt coalescing）</h5>

<p>中断合并会将多个中断事件放到一起，到达一定的阈值之后才向CPU发起中断请求。</p>

<p>这可以防止中断风暴，提升吞吐。减少中断数量能使吞吐更高，但延迟也变大，CPU使用量
下降；中断数量过多则相反。</p>

<p>历史上，早期的igb、e1000版本，以及其他的都包含一个叫InterruptThrottleRate参数，
最近的版本已经被ethtool可配置的参数取代。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-c</span> eth0
Coalesce parameters <span class="k">for </span>eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0
...
</code></pre></div></div>

<p>ethtool提供了用于中断合并相关的通用的接口。但切记，不是所有的设备都支持完整的配
置。你需要查看你的驱动文档或代码来确定哪些支持，哪些不支持。ethtool的文档说的：“
驱动没有实现的接口将会被静默忽略”。</p>

<p>某些驱动支持一个有趣的特性“自适应 RX/TX 硬中断合并”。这个特性一般是在硬件实现的
。驱动通常需要做一些额外的工作来告诉网卡需要打开这个特性（前面的igb驱动代码里有
涉及）。</p>

<p>自适应RX/TX硬中断合并带来的效果是：带宽比较低时降低延迟，带宽比较高时提升吞吐。</p>

<p>用<code class="highlighter-rouge">ethtool -C</code>打开自适应RX IRQ合并：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-C</span> eth0 adaptive-rx on
</code></pre></div></div>

<p>还可以用<code class="highlighter-rouge">ethtool -C</code>更改其他配置。常用的包括：</p>

<ul>
  <li><code class="highlighter-rouge">rx-usecs</code>: How many usecs to delay an RX interrupt after a packet arrives.</li>
  <li><code class="highlighter-rouge">rx-frames</code>: Maximum number of data frames to receive before an RX interrupt.</li>
  <li><code class="highlighter-rouge">rx-usecs-irq</code>: How many usecs to delay an RX interrupt while an interrupt is being serviced by the host.</li>
  <li><code class="highlighter-rouge">rx-frames-irq</code>: Maximum number of data frames to receive before an RX interrupt is generated while the system is servicing an interrupt.</li>
</ul>

<p>请注意你的硬件可能只支持以上列表的一个子集，具体请参考相应的驱动说明或源码。</p>

<p>不幸的是，通常并没有一个很好的文档来说明这些选项，最全的文档很可能是头文件。查看
<a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/ethtool.h#L184-L255">include/uapi/linux/ethtool.h</a> ethtool每个每个选项的解释。</p>

<p>注意：虽然硬中断合并看起来是个不错的优化项，但要你的网络栈的其他一些相应
部分也要针对性的调整。只合并硬中断很可能并不会带来多少收益。</p>

<h5 id="调整硬中断亲和性irq-affinities">调整硬中断亲和性（IRQ affinities）</h5>

<p>If your NIC supports RSS / multiqueue or if you are attempting to optimize for data locality, you may wish to use a specific set of CPUs for handling interrupts generated by your NIC.</p>

<p>Setting specific CPUs allows you to segment which CPUs will be used for processing which IRQs. These changes may affect how upper layers operate, as we’ve seen for the networking stack.</p>

<p>If you do decide to adjust your IRQ affinities, you should first check if you running the irqbalance daemon. This daemon tries to automatically balance IRQs to CPUs and it may overwrite your settings. If you are running irqbalance, you should either disable irqbalance or use the –banirq in conjunction with IRQBALANCE_BANNED_CPUS to let irqbalance know that it shouldn’t touch a set of IRQs and CPUs that you want to assign yourself.</p>

<p>Next, you should check the file /proc/interrupts for a list of the IRQ numbers for each network RX queue for your NIC.</p>

<p>Finally, you can adjust the which CPUs each of those IRQs will be handled by modifying /proc/irq/IRQ_NUMBER/smp_affinity for each IRQ number.</p>

<p>You simply write a hexadecimal bitmask to this file to instruct the kernel which CPUs it should use for handling the IRQ.</p>

<p>Example: Set the IRQ affinity for IRQ 8 to CPU 0</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>bash <span class="nt">-c</span> <span class="s1">'echo 1 &gt; /proc/irq/8/smp_affinity'</span>
</code></pre></div></div>

<h3 id="53-网络数据处理开始">5.3 网络数据处理：开始</h3>

<p>一旦软中断代码判断出有softirq处于pending状态，就会开始处理，执行<code class="highlighter-rouge">net_rx_action</code>，网络数据处理就此开始。</p>

<p>我们来看一下<code class="highlighter-rouge">net_rx_action</code> 的循环部分，理解它是如何工作的。哪个部分可以调优，哪个可以被监控。</p>

<h4 id="531-net_rx_action处理循环">5.3.1 <code class="highlighter-rouge">net_rx_action</code>处理循环</h4>

<p><code class="highlighter-rouge">net_rx_action</code>从包所在的内存开始处理，包是被设备通过DMA直接送到内存的。</p>

<p>函数遍历本CPU队列的NAPI实例列表，依次出队，操作它。</p>

<p>处理逻辑考虑任务量（work）和执行时间两个因素：</p>

<ol>
  <li>跟踪记录工作量预算（work budget），预算可以调整</li>
  <li>记录消耗的时间</li>
</ol>

<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4380-L4383">net/core/dev.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">list_empty</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sd</span><span class="o">-&gt;</span><span class="n">poll_list</span><span class="p">))</span> <span class="p">{</span>
    <span class="k">struct</span> <span class="n">napi_struct</span> <span class="o">*</span><span class="n">n</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">work</span><span class="p">,</span> <span class="n">weight</span><span class="p">;</span>

    <span class="cm">/* If softirq window is exhausted then punt.
     * Allow this to run for 2 jiffies since which will allow
     * an average latency of 1.5/HZ.
     */</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">budget</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">time_after_eq</span><span class="p">(</span><span class="n">jiffies</span><span class="p">,</span> <span class="n">time_limit</span><span class="p">)))</span>
      <span class="k">goto</span> <span class="n">softnet_break</span><span class="p">;</span>
</code></pre></div></div>

<p>这里可以看到内核是如何防止处理数据包过程霸占整个CPU的，其中budget是该CPU的所有
NAPI实例的总预算。</p>

<p>这也是多队列网卡应该精心调整IRQ Affinity的原因。回忆前面讲的，处理硬中断的CPU接
下来会处理相应的软中断，进而执行上面包含budget的这段逻辑。</p>

<p>多网卡多队列可能会出现这样的情况：多个NAPI实例注册到同一个CPU上。每个CPU上的所有
NAPI实例共享一份budget。</p>

<p>如果你没有足够的CPU来分散网卡硬中断，可以考虑增加<code class="highlighter-rouge">net_rx_action</code>允许每个CPU处理
更多包。增加budget可以增加CPU使用量（<code class="highlighter-rouge">top</code>等命令看到的<code class="highlighter-rouge">sitime</code>或<code class="highlighter-rouge">si</code>部分），
但可以减少延迟，因为数据处理更加及时。</p>

<p>Note: the CPU will still be bounded by a time limit of 2 jiffies, regardless of the assigned budget.</p>

<h4 id="532-napi-poll-function-and-weight">5.3.2 NAPI poll function and weight</h4>

<p>回忆前面，网络设备驱动使用<code class="highlighter-rouge">netif_napi_add</code>注册poll方法，<code class="highlighter-rouge">igb</code>驱动有如下
代码片段：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="cm">/* initialize NAPI */</span>
  <span class="n">netif_napi_add</span><span class="p">(</span><span class="n">adapter</span><span class="o">-&gt;</span><span class="n">netdev</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">q_vector</span><span class="o">-&gt;</span><span class="n">napi</span><span class="p">,</span> <span class="n">igb_poll</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>
</code></pre></div></div>

<p>这注册了一个NAPI实例，hardcode 64的权重。我们来看在<code class="highlighter-rouge">net_rx_action</code>处理循环中这个
值是如何使用的。
<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4322-L4338">net/core/dev.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weight</span> <span class="o">=</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">weight</span><span class="p">;</span>

<span class="n">work</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">test_bit</span><span class="p">(</span><span class="n">NAPI_STATE_SCHED</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">n</span><span class="o">-&gt;</span><span class="n">state</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">work</span> <span class="o">=</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">poll</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">weight</span><span class="p">);</span>
        <span class="n">trace_napi_poll</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">WARN_ON_ONCE</span><span class="p">(</span><span class="n">work</span> <span class="o">&gt;</span> <span class="n">weight</span><span class="p">);</span>

<span class="n">budget</span> <span class="o">-=</span> <span class="n">work</span><span class="p">;</span>
</code></pre></div></div>

<p>其中的<code class="highlighter-rouge">n</code>是<code class="highlighter-rouge">struct napi</code>的实例。其中的<code class="highlighter-rouge">poll</code>指向<code class="highlighter-rouge">igb_poll</code>。<code class="highlighter-rouge">poll()</code>返回处理的数
据帧数量，budget会减去这个值。</p>

<p>所以，假设你的驱动使用weight值64（Linux 3.13.0 的所有驱动都是hardcode这个值）
，设置budget默认值300，那你的系统将在如下条件之一停止数据处理：</p>

<ol>
  <li><code class="highlighter-rouge">igb_poll</code>函数被调用了最多5次（如果没有数据需要处理，那次数就会很少）</li>
  <li>时间经过了至少2个jiffies</li>
</ol>

<h4 id="533-napi和设备驱动的合约">5.3.3 NAPI和设备驱动的合约</h4>

<p>NAPI子系统和设备驱动之间的合约，最重要的一点是关闭NAPI的条件。具体如下：</p>

<ol>
  <li>如果驱动的<code class="highlighter-rouge">poll</code>方法用完了它的全部weight（默认hardcode 64），那它<strong>不要更改</strong>NAPI
状态。接下来<code class="highlighter-rouge">net_rx_action</code> loop会做的</li>
  <li>如果驱动的<code class="highlighter-rouge">poll</code>方法没有用完全部weight，那它<strong>必须关闭</strong>NAPI。下次有硬件中断触
发，驱动的硬件处理函数调用<code class="highlighter-rouge">napi_schedule</code>时，NAPI会被重新打开</li>
</ol>

<p>接下来先看<code class="highlighter-rouge">net_rx_action</code>如何处理合约的第一部分，然后看<code class="highlighter-rouge">poll</code>方法如何处理第二部
分。</p>

<h4 id="534-finishing-the-net_rx_action-loop">5.3.4 Finishing the <code class="highlighter-rouge">net_rx_action</code> loop</h4>

<p><code class="highlighter-rouge">net_rx_action</code>循环的基础部分，处理NAPI合约的第一部分。
<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4342-L4363">net/core/dev.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Drivers must not modify the NAPI state if they
 * consume the entire weight.  In such cases this code
 * still "owns" the NAPI instance and therefore can
 * move the instance around on the list at-will.
 */</span>
<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">work</span> <span class="o">==</span> <span class="n">weight</span><span class="p">))</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">napi_disable_pending</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span> <span class="p">{</span>
    <span class="n">local_irq_enable</span><span class="p">();</span>
    <span class="n">napi_complete</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>
    <span class="n">local_irq_disable</span><span class="p">();</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">-&gt;</span><span class="n">gro_list</span><span class="p">)</span> <span class="p">{</span>
      <span class="cm">/* flush too old packets
       * If HZ &lt; 1000, flush all packets.
       */</span>
      <span class="n">local_irq_enable</span><span class="p">();</span>
      <span class="n">napi_gro_flush</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">HZ</span> <span class="o">&gt;=</span> <span class="mi">1000</span><span class="p">);</span>
      <span class="n">local_irq_disable</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="n">list_move_tail</span><span class="p">(</span><span class="o">&amp;</span><span class="n">n</span><span class="o">-&gt;</span><span class="n">poll_list</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sd</span><span class="o">-&gt;</span><span class="n">poll_list</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>如果全部<code class="highlighter-rouge">work</code>都用完了，<code class="highlighter-rouge">net_rx_action</code>会有两种情况需要处理：</p>

<ol>
  <li>网络设备需要关闭（例如，用户敲了<code class="highlighter-rouge">ifconfig eth0 down</code>命令）</li>
  <li>如果设备不需要关闭，那检查是否有GRO（后面会介绍）列表。如果时钟tick rate <code class="highlighter-rouge">&gt;=
1000</code>，所有最近被更新的GRO network flow都会被flush。将这个NAPI实例移到list末
尾，这个循环下次再进入时，处理的就是下一个NAPI实例</li>
</ol>

<p>这就是包处理循环如何唤醒驱动注册的<code class="highlighter-rouge">poll</code>方法进行包处理的过程。接下来会看到，
<code class="highlighter-rouge">poll</code>方法会收割网络数据，发送到上层栈进行处理。</p>

<h4 id="535-到达limit时退出循环">5.3.5 到达limit时退出循环</h4>

<p><code class="highlighter-rouge">net_rx_action</code>下列条件之一退出循环：</p>

<ol>
  <li>这个CPU上注册的poll列表已经没有NAPI实例需要处理(<code class="highlighter-rouge">!list_empty(&amp;sd-&gt;poll_list)</code>)</li>
  <li>剩余的<code class="highlighter-rouge">budget &lt;= 0</code></li>
  <li>已经满足2个jiffies的时间限制</li>
</ol>

<p>代码：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* If softirq window is exhausted then punt.
 * Allow this to run for 2 jiffies since which will allow
 * an average latency of 1.5/HZ.
 */</span>
<span class="k">if</span> <span class="p">(</span><span class="n">unlikely</span><span class="p">(</span><span class="n">budget</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="o">||</span> <span class="n">time_after_eq</span><span class="p">(</span><span class="n">jiffies</span><span class="p">,</span> <span class="n">time_limit</span><span class="p">)))</span>
  <span class="k">goto</span> <span class="n">softnet_break</span><span class="p">;</span>
</code></pre></div></div>

<p>如果跟踪<code class="highlighter-rouge">softnet_break</code>，会发现很有意思的东西：</p>

<p>From net/core/dev.c:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nl">softnet_break:</span>
  <span class="n">sd</span><span class="o">-&gt;</span><span class="n">time_squeeze</span><span class="o">++</span><span class="p">;</span>
  <span class="n">__raise_softirq_irqoff</span><span class="p">(</span><span class="n">NET_RX_SOFTIRQ</span><span class="p">);</span>
  <span class="k">goto</span> <span class="n">out</span><span class="p">;</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">softnet_data</code>实例更新统计信息，软中断的<code class="highlighter-rouge">NET_RX_SOFTIRQ</code>被关闭。</p>

<p><code class="highlighter-rouge">time_squeeze</code>字段记录的是满足如下条件的次数：<code class="highlighter-rouge">net_rx_action</code>有很多<code class="highlighter-rouge">work</code>要做但
是bugdget用完了，或者work还没做完但时间限制到了。这对理解网络处理的瓶颈至关重要
。我们后面会看到如何监控这个值。关闭<code class="highlighter-rouge">NET_RX_SOFTIRQ</code>是为了释放CPU时间给其他任务
用。这行代码是有意义的，因为只有我们有更多工作要做（还没做完）的时候才会执行到这里，
我们主动让出CPU，不想独占太久。</p>

<p>然后执行到了<code class="highlighter-rouge">out</code>标签所在的代码。另外一种条件也会跳转到<code class="highlighter-rouge">out</code>标签：所有NAPI实例都
处理完了，换言之，budget数量大于网络包数量，所有驱动都已经关闭NAPI，没有什么事情
需要<code class="highlighter-rouge">net_rx_action</code>做了。</p>

<p><code class="highlighter-rouge">out</code>代码段在从<code class="highlighter-rouge">net_rx_action</code>返回之前做了一件重要的事情：调用
<code class="highlighter-rouge">net_rps_action_and_irq_enable</code>。Receive Packet Steering功能打开的时候，这个函数
有重要作用：唤醒其他CPU处理网络包。</p>

<p>我们后面会看到RPS是如何工作的。现在，让我们看看怎样监控<code class="highlighter-rouge">net_rx_action</code>处理循环的
健康状态，以及进入NAPI <code class="highlighter-rouge">poll</code>的内部，这样才能更好的理解网络栈。</p>

<h4 id="356-napi-poll">3.5.6 NAPI <code class="highlighter-rouge">poll</code></h4>

<p>回忆前文，驱动程序会分配一段内存用于DMA，将数据包写到内存。就像这段内存是由驱动
程序分配的一样，驱动程序也负责解绑（unmap）这些内存，读取数据，将数据送到网络栈
。</p>

<p>我们看下<code class="highlighter-rouge">igb</code>驱动如何实现这一过程的。</p>

<h5 id="igb_poll"><code class="highlighter-rouge">igb_poll</code></h5>

<p>可以看到<code class="highlighter-rouge">igb_poll</code>代码其实相当简单。
<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
 *  igb_poll - NAPI Rx polling callback
 *  @napi: napi polling structure
 *  @budget: count of how many packets we should handle
 **/</span>
<span class="k">static</span> <span class="kt">int</span> <span class="nf">igb_poll</span><span class="p">(</span><span class="k">struct</span> <span class="n">napi_struct</span> <span class="o">*</span><span class="n">napi</span><span class="p">,</span> <span class="kt">int</span> <span class="n">budget</span><span class="p">)</span>
<span class="p">{</span>
        <span class="k">struct</span> <span class="n">igb_q_vector</span> <span class="o">*</span><span class="n">q_vector</span> <span class="o">=</span> <span class="n">container_of</span><span class="p">(</span><span class="n">napi</span><span class="p">,</span>
                                                     <span class="k">struct</span> <span class="n">igb_q_vector</span><span class="p">,</span>
                                                     <span class="n">napi</span><span class="p">);</span>
        <span class="n">bool</span> <span class="n">clean_complete</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>

<span class="cp">#ifdef CONFIG_IGB_DCA
</span>        <span class="k">if</span> <span class="p">(</span><span class="n">q_vector</span><span class="o">-&gt;</span><span class="n">adapter</span><span class="o">-&gt;</span><span class="n">flags</span> <span class="o">&amp;</span> <span class="n">IGB_FLAG_DCA_ENABLED</span><span class="p">)</span>
                <span class="n">igb_update_dca</span><span class="p">(</span><span class="n">q_vector</span><span class="p">);</span>
<span class="cp">#endif
</span>
        <span class="cm">/* ... */</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">q_vector</span><span class="o">-&gt;</span><span class="n">rx</span><span class="p">.</span><span class="n">ring</span><span class="p">)</span>
                <span class="n">clean_complete</span> <span class="o">&amp;=</span> <span class="n">igb_clean_rx_irq</span><span class="p">(</span><span class="n">q_vector</span><span class="p">,</span> <span class="n">budget</span><span class="p">);</span>

        <span class="cm">/* If all work not completed, return budget and keep polling */</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">clean_complete</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">budget</span><span class="p">;</span>

        <span class="cm">/* If not enough Rx work done, exit the polling mode */</span>
        <span class="n">napi_complete</span><span class="p">(</span><span class="n">napi</span><span class="p">);</span>
        <span class="n">igb_ring_irq_enable</span><span class="p">(</span><span class="n">q_vector</span><span class="p">);</span>

        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>几件有意思的事情：</p>

<ul>
  <li>如果内核<a href="https://lwn.net/Articles/247493/">DCA</a>（Direct Cache Access）功能打开了，CPU缓存是热的，对RX ring的访问会
命中CPU cache。更多DCA信息见本文“Extra”部分。</li>
  <li>然后执行<code class="highlighter-rouge">igb_clean_rx_irq</code>，这里做的事情非常多，我们后面看</li>
  <li>然后执行<code class="highlighter-rouge">clean_complete</code>，判断是否仍然有work可以做。如果有，就返回budget（回忆
，这里是hardcode 64）。在之前我们已经看到，<code class="highlighter-rouge">net_rx_action</code>会将这个NAPI实例移动
到poll列表的末尾</li>
  <li>如果所有<code class="highlighter-rouge">work</code>都已经完成，驱动通过调用<code class="highlighter-rouge">napi_complete</code>关闭NAPI，并通过调用
<code class="highlighter-rouge">igb_ring_irq_enable</code>重新进入可中断状态。下次中断到来的时候回重新打开NAPI</li>
</ul>

<p>我们来看<code class="highlighter-rouge">igb_clean_rx_irq</code>如何将网络数据送到网络栈。</p>

<h5 id="igb_clean_rx_irq"><code class="highlighter-rouge">igb_clean_rx_irq</code></h5>

<p><code class="highlighter-rouge">igb_clean_rx_irq</code>方法是一个循环，每次处理一个包，直到budget用完，或者没有数据需要处理了。</p>

<p>做的几件重要事情：</p>

<ol>
  <li>分配额外的buffer用于接收数据，因为已经用过的buffer被clean out了。一次分配<code class="highlighter-rouge">IGB_RX_BUFFER_WRITE (16)</code>个。</li>
  <li>从RX队列取一个buffer，保存到一个<code class="highlighter-rouge">skb</code>类型的实例中</li>
  <li>判断这个buffer是不是一个包的最后一个buffer。如果是，继续处理；如果不是，继续
从buffer列表中拿出下一个buffer，加到skb。当数据帧的大小比一个buffer大的时候，
会出现这种情况</li>
  <li>验证数据的layout和头信息是正确的</li>
  <li>更新<code class="highlighter-rouge">skb-&gt;len</code>，表示这个包已经处理的字节数</li>
  <li>设置<code class="highlighter-rouge">skb</code>的hash, checksum, timestamp, VLAN id, protocol字段。hash，
checksum，timestamp，VLAN ID信息是硬件提供的，如果硬件报告checksum error，
<code class="highlighter-rouge">csum_error</code>统计就会增加。如果checksum通过了，数据是UDP或者TCP数据，<code class="highlighter-rouge">skb</code>就会
被标记成<code class="highlighter-rouge">CHECKSUM_UNNECESSARY</code></li>
  <li>构建的skb经<code class="highlighter-rouge">napi_gro_receive()</code>进入协议栈</li>
  <li>更新处理过的包的统计信息</li>
  <li>循环直至处理的包数量达到budget</li>
</ol>

<p>循环结束的时候，这个函数设置收包的数量和字节数统计信息。</p>

<p>接下来在进入协议栈之前，我们先开两个小差：首先是看一些如何监控和调优软中断，其次
是介绍GRO。有了这个两个背景，后面（通过<code class="highlighter-rouge">napi_gro_receive</code>进入）协议栈部分会更容易理解。</p>

<h4 id="536-监控网络数据处理">5.3.6 监控网络数据处理</h4>

<h5 id="procnetsoftnet_stat"><code class="highlighter-rouge">/proc/net/softnet_stat</code></h5>

<p>前面看到，如果budget或者time limit到了而仍有包需要处理，那<code class="highlighter-rouge">net_rx_action</code>在退出
循环之前会更新统计信息。这个信息存储在该CPU的<code class="highlighter-rouge">struct softnet_data</code>实例中。</p>

<p>这些统计信息打到了<code class="highlighter-rouge">/proc/net/softnet_stat</code>，但不幸的是，关于这个的文档很少。每一
列代表什么并没有标题，而且列的内容会随着内核版本可能发生变化。</p>

<p>在内核 3.13.0中，你可以阅读内核源码，查看每一列分别对应什么。
<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/net-procfs.c#L161-L165">net/core/net-procfs.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">seq_printf</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span>
       <span class="s">"%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
       <span class="n">sd</span><span class="o">-&gt;</span><span class="n">processed</span><span class="p">,</span> <span class="n">sd</span><span class="o">-&gt;</span><span class="n">dropped</span><span class="p">,</span> <span class="n">sd</span><span class="o">-&gt;</span><span class="n">time_squeeze</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
       <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="cm">/* was fastroute */</span>
       <span class="n">sd</span><span class="o">-&gt;</span><span class="n">cpu_collision</span><span class="p">,</span> <span class="n">sd</span><span class="o">-&gt;</span><span class="n">received_rps</span><span class="p">,</span> <span class="n">flow_limit_count</span><span class="p">);</span>
</code></pre></div></div>

<p>其中一些的名字让人很困惑，而且在你意想不到的地方更新。在接下来的网络栈分析说，我
们会举例说明其中一些字段是何时、在哪里被更新的。前面我们已经看到了<code class="highlighter-rouge">squeeze_time</code>
是在<code class="highlighter-rouge">net_rx_action</code>在被更新的，到此时，如下数据你应该能看懂了：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /proc/net/softnet_stat
6dcad223 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6f0e1565 00000000 00000002 00000000 00000000 00000000 00000000 00000000 00000000 00000000
660774ec 00000000 00000003 00000000 00000000 00000000 00000000 00000000 00000000 00000000
61c99331 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6794b1b3 00000000 00000005 00000000 00000000 00000000 00000000 00000000 00000000 00000000
6488cb92 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
</code></pre></div></div>

<p>关于<code class="highlighter-rouge">/proc/net/softnet_stat</code>的重要细节:</p>

<ol>
  <li>每一行代表一个<code class="highlighter-rouge">struct softnet_data</code>实例。因为每个CPU只有一个该实例，所以每行
其实代表一个CPU</li>
  <li>每列用空格隔开，数值用16进制表示</li>
  <li>第一列 <code class="highlighter-rouge">sd-&gt;processed</code>，是处理的网络帧的数量。如果你使用了ethernet bonding，
那这个值会大于总的网络帧的数量，因为ethernet bonding驱动有时会触发网络数据被
重新处理（re-processed）</li>
  <li>第二列，<code class="highlighter-rouge">sd-&gt;dropped</code>，是因为处理不过来而drop的网络帧数量。后面会展开这一话题</li>
  <li>第三列，<code class="highlighter-rouge">sd-&gt;time_squeeze</code>，前面介绍过了，由于budget或time limit用完而退出
<code class="highlighter-rouge">net_rx_action</code>循环的次数</li>
  <li>接下来的5列全是0</li>
  <li>第九列，<code class="highlighter-rouge">sd-&gt;cpu_collision</code>，是为了发送包而获取锁的时候有冲突的次数</li>
  <li>第十列，<code class="highlighter-rouge">sd-&gt;received_rps</code>，是这个CPU被其他CPU唤醒去收包的次数</li>
  <li>最后一列，<code class="highlighter-rouge">flow_limit_count</code>，是达到flow limit的次数。flow limit是RPS的特性，
后面会稍微介绍一下</li>
</ol>

<p>如果你要画图监控这些数据，确保你的列和相应的字段是对的上的，最保险的方式是阅读相
应版本的内核代码。</p>

<h4 id="537-网络数据处理调优">5.3.7 网络数据处理调优</h4>

<h5 id="调整net_rx_action-budget">调整<code class="highlighter-rouge">net_rx_action</code> budget</h5>

<p><code class="highlighter-rouge">net_rx_action</code> budget表示一个CPU单次轮询（<code class="highlighter-rouge">poll</code>）所允许的最大收包数量。单次
poll收包是，所有注册到这个CPU的NAPI实例收包数量之和不能大于这个阈值。 调整：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>sysctl <span class="nt">-w</span> net.core.netdev_budget<span class="o">=</span>600
</code></pre></div></div>

<p>如果要保证重启仍然生效，需要将这个配置写到<code class="highlighter-rouge">/etc/sysctl.conf</code>。</p>

<p>Linux 3.13.0的默认配置是300。</p>

<h3 id="54-grogeneric-receive-offloading">5.4 GRO（Generic Receive Offloading）</h3>

<p>Large Receive Offloading (LRO) 是一个硬件优化，GRO是LRO的一种软件实现。</p>

<p>两种方案的主要思想都是：通过合并“足够类似”的包来减少往网络栈传送的包的数量，这有
助于减少CPU的使用量。例如，考虑大文件传输的场景，包的数量非常多，大部分包都是一
段文件数据。相比于每次都将小包送到网络栈，可以将收到的小包合并成一个很大的包再送
到网络栈。这可以使得协议层只需要处理一个header，而将包含大量数据的整个大包送到用
户程序。</p>

<p>这类优化方式的缺点就是：信息丢失。如果一个包有一些重要的option或者flag，那将这个
包的数据合并到其他包时，这些信息就会丢失。这也是为什么大部分人不使用或不推荐使用
LRO的原因。</p>

<p>LRO的实现，一般来说，对合并包的规则非常宽松。GRO是LRO的软件实现，但是对于包合并
的规则更严苛。</p>

<p>顺便说一下，如果你曾经用过tcpdump抓包，并收到看起来不现实的非常大的包，那很可能
是你的系统开启了GRO。你接下来会看到，捕获包的tap在整个栈的更后面一下，在GRO之
后。</p>

<h4 id="使用ethtool修改gro配置">使用ethtool修改GRO配置</h4>

<p><code class="highlighter-rouge">-k</code>查看GRO配置：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ethtool <span class="nt">-k</span> eth0 | <span class="nb">grep </span>generic-receive-offload
generic-receive-offload: on
</code></pre></div></div>

<p><code class="highlighter-rouge">-K</code>修改GRO配置：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-K</span> eth0 gro on
</code></pre></div></div>

<p>注意：对于大部分驱动，修改GRO配置会涉及先down再up这个网卡，因此这个网卡上的连接
都会中断。</p>

<h3 id="55-napi_gro_receive">5.5 <code class="highlighter-rouge">napi_gro_receive</code></h3>

<p>如果开启了GRO，<code class="highlighter-rouge">napi_gro_receive</code>将负责处理网络数据，并将数据送到协议栈，大部分
相关的逻辑在函数<code class="highlighter-rouge">dev_gro_receive</code>里实现。</p>

<h4 id="dev_gro_receive"><code class="highlighter-rouge">dev_gro_receive</code></h4>

<p>这个函数首先检查GRO是否开启了，如果是，就准备做GRO。GRO首先遍历一个offload
filter列表，如果高层协议认为其中一些数据属于GRO处理的范围，就会允许其对数据进行
操作。</p>

<p>协议层以此方式让网络设备层知道，这个packet是不是当前正在处理的一个需要做GRO的
network flow的一部分，而且也可以通过这种方式传递一些协议相关的信息。例如，TCP协
议需要判断是否以及合适应该将一个ACK包合并到其他包里。</p>

<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3844-L3856">net/core/dev.c</a>:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">list_for_each_entry_rcu</span><span class="p">(</span><span class="n">ptype</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">list</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ptype</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">!=</span> <span class="n">type</span> <span class="o">||</span> <span class="o">!</span><span class="n">ptype</span><span class="o">-&gt;</span><span class="n">callbacks</span><span class="p">.</span><span class="n">gro_receive</span><span class="p">)</span>
    <span class="k">continue</span><span class="p">;</span>

  <span class="n">skb_set_network_header</span><span class="p">(</span><span class="n">skb</span><span class="p">,</span> <span class="n">skb_gro_offset</span><span class="p">(</span><span class="n">skb</span><span class="p">));</span>
  <span class="n">skb_reset_mac_len</span><span class="p">(</span><span class="n">skb</span><span class="p">);</span>
  <span class="n">NAPI_GRO_CB</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">same_flow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">NAPI_GRO_CB</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">flush</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">NAPI_GRO_CB</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">free</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

  <span class="n">pp</span> <span class="o">=</span> <span class="n">ptype</span><span class="o">-&gt;</span><span class="n">callbacks</span><span class="p">.</span><span class="n">gro_receive</span><span class="p">(</span><span class="o">&amp;</span><span class="n">napi</span><span class="o">-&gt;</span><span class="n">gro_list</span><span class="p">,</span> <span class="n">skb</span><span class="p">);</span>
  <span class="k">break</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>如果协议层提示是时候flush GRO packet了，那就到下一步处理了。这发生在
<code class="highlighter-rouge">napi_gro_complete</code>，会进一步调用相应协议的<code class="highlighter-rouge">gro_complete</code>回调方法，然后调用
<code class="highlighter-rouge">netif_receive_skb</code>将包送到协议栈。
这个过程见<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3862-L3872">net/core/dev.c</a>：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">pp</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">struct</span> <span class="n">sk_buff</span> <span class="o">*</span><span class="n">nskb</span> <span class="o">=</span> <span class="o">*</span><span class="n">pp</span><span class="p">;</span>

  <span class="o">*</span><span class="n">pp</span> <span class="o">=</span> <span class="n">nskb</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
  <span class="n">nskb</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
  <span class="n">napi_gro_complete</span><span class="p">(</span><span class="n">nskb</span><span class="p">);</span>
  <span class="n">napi</span><span class="o">-&gt;</span><span class="n">gro_count</span><span class="o">--</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>接下来，如果协议层将这个包合并到一个已经存在的flow，<code class="highlighter-rouge">napi_gro_receive</code>就没什么事
情需要做，因此就返回了。如果packet没有被合并，而且GRO的数量小于 <code class="highlighter-rouge">MAX_GRO_SKBS</code>（
默认是8），就会创建一个新的entry加到本CPU的NAPI实例的<code class="highlighter-rouge">gro_list</code>。
<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3877-L3886">net/core/dev.c</a>：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">NAPI_GRO_CB</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">flush</span> <span class="o">||</span> <span class="n">napi</span><span class="o">-&gt;</span><span class="n">gro_count</span> <span class="o">&gt;=</span> <span class="n">MAX_GRO_SKBS</span><span class="p">)</span>
  <span class="k">goto</span> <span class="n">normal</span><span class="p">;</span>

<span class="n">napi</span><span class="o">-&gt;</span><span class="n">gro_count</span><span class="o">++</span><span class="p">;</span>
<span class="n">NAPI_GRO_CB</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">count</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">NAPI_GRO_CB</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">age</span> <span class="o">=</span> <span class="n">jiffies</span><span class="p">;</span>
<span class="n">skb_shinfo</span><span class="p">(</span><span class="n">skb</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">gso_size</span> <span class="o">=</span> <span class="n">skb_gro_len</span><span class="p">(</span><span class="n">skb</span><span class="p">);</span>
<span class="n">skb</span><span class="o">-&gt;</span><span class="n">next</span> <span class="o">=</span> <span class="n">napi</span><span class="o">-&gt;</span><span class="n">gro_list</span><span class="p">;</span>
<span class="n">napi</span><span class="o">-&gt;</span><span class="n">gro_list</span> <span class="o">=</span> <span class="n">skb</span><span class="p">;</span>
<span class="n">ret</span> <span class="o">=</span> <span class="n">GRO_HELD</span><span class="p">;</span>
</code></pre></div></div>

<p>这就是Linux网络栈中GRO的工作原理。</p>

<h3 id="56-napi_skb_finish">5.6 <code class="highlighter-rouge">napi_skb_finish</code></h3>

<p>一旦<code class="highlighter-rouge">dev_gro_receive</code>完成，<code class="highlighter-rouge">napi_skb_finish</code>就会被调用，其如果一个packet被合并了
，就释放不用的变量；或者调用<code class="highlighter-rouge">netif_receive_skb</code>将数据发送到网络协议栈（因为已经
有<code class="highlighter-rouge">MAX_GRO_SKBS</code>个flow了，够GRO了）。</p>

<p>接下来，是看<code class="highlighter-rouge">netif_receive_skb</code>如何将数据交给协议层。但在此之前，我们先看一下RPS。</p>

<h2 id="6-rps-receive-packet-steering">6 RPS (Receive Packet Steering)</h2>

<p>回忆前面我们讨论了网络设备驱动是如何注册NAPI <code class="highlighter-rouge">poll</code>方法的。每个NAPI实例都会运
行在相应CPU的软中断的上下文中。而且，触发硬中断的这个CPU接下来会负责执行相应的软
中断处理函数来收包。</p>

<p>换言之，同一个CPU既处理硬中断，又处理相应的软中断。</p>

<p>一些网卡（例如Intel I350）在硬件层支持多队列。这意味着收进来的包会被通过DMA放到
位于不同内存的队列上，而不同的队列有相应的NAPI实例管理软中断<code class="highlighter-rouge">poll()</code>过程。因此，
多个CPU同时处理从网卡来的中断，处理收包过程。</p>

<p>这个特性被称作RSS（Receive Side Scaling，接收端扩展）。</p>

<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222">RPS</a>
（Receive Packet Steering，接收包控制，接收包引导）是RSS的一种软件实现。因为是软
件实现的，意味着任何网卡都可以使用这个功能，即便是那些只有一个接收队列的网卡。但
是，因为它是软件实现的，这意味着RPS只能在packet通过DMA进入内存后，RPS才能开始工
作。</p>

<p>这意味着，RPS并不会减少CPU处理硬件中断和NAPI <code class="highlighter-rouge">poll</code>（软中断最重要的一部分）的时
间，但是可以在packet到达内存后，将packet分到其他CPU，从其他CPU进入协议栈。</p>

<p>RPS的工作原理是对个packet做hash，以此决定分到哪个CPU处理。然后packet放到每个CPU
独占的接收后备队列（backlog）等待处理。这个CPU会触发一个进程间中断（
<a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">IPI</a>，Inter-processor
Interrupt）向对端CPU。如果当时对端CPU没有在处理backlog队列收包，这个进程间中断会
触发它开始从backlog收包。<code class="highlighter-rouge">/proc/net/softnet_stat</code>其中有一列是记录<code class="highlighter-rouge">softnet_data</code>
实例（也即这个CPU）收到了多少IPI（<code class="highlighter-rouge">received_rps</code>列）。</p>

<p>因此，<code class="highlighter-rouge">netif_receive_skb</code>或者继续将包送到协议栈，或者交给RPS，后者会转交给其他CPU处理。</p>

<h3 id="rps调优">RPS调优</h3>

<p>使用RPS需要在内核做配置（Ubuntu + Kernel 3.13.0 支持），而且需要一个掩码（
bitmask）指定哪些CPU可以处理那些RX队列。相关的一些信息可以在<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L138-L164">内核文档
</a>
里找到。</p>

<p>bitmask配置位于：<code class="highlighter-rouge">/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus</code>。</p>

<p>例如，对于eth0的queue 0，你需要更改<code class="highlighter-rouge">/sys/class/net/eth0/queues/rx-0/rps_cpus</code>。<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L160-L164">
内核文档
</a>
里说，对一些特定的配置下，RPS没必要了。</p>

<p>注意：打开RPS之后，原来不需要处理软中断（softirq）的CPU这时也会参与处理。因此相
应CPU的<code class="highlighter-rouge">NET_RX</code>数量，以及<code class="highlighter-rouge">si</code>或<code class="highlighter-rouge">sitime</code>占比都会相应增加。你可以对比启用RPS前后的
数据，以此来确定你的配置是否生效，以及是否符合预期（哪个CPU处理哪个网卡的哪个中
断）。</p>

<h2 id="7-rfs-receive-flow-steering">7 RFS (Receive Flow Steering)</h2>

<p>RFS（Receive flow steering）和RPS配合使用。RPS试图在CPU之间平衡收包，但是没考虑
数据的本地性问题，如何最大化CPU缓存的命中率。RFS将属于相同flow的包送到相同的CPU
进行处理，可以提高缓存命中率。</p>

<h3 id="调优打开rfs">调优：打开RFS</h3>

<p>RPS记录一个全局的hash table，包含所有flow的信息。这个hash table的大小可以在<code class="highlighter-rouge">net.core.rps_sock_flow_entries</code>：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo sysctl -w net.core.rps_sock_flow_entries=32768
</code></pre></div></div>

<p>其次，你可以设置每个RX queue的flow数量，对应着<code class="highlighter-rouge">rps_flow_cnt</code>：</p>

<p>例如，eth0的RX queue0的flow数量调整到2048：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo bash -c 'echo 2048 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt'
</code></pre></div></div>

<h2 id="8-arfs-hardware-accelerated-rfs">8 aRFS (Hardware accelerated RFS)</h2>

<p>RFS可以用硬件加速，网卡和内核协同工作，判断哪个flow应该在哪个CPU上处理。这需要网
卡和网卡驱动的支持。</p>

<p>如果你的网卡驱动里对外提供一个<code class="highlighter-rouge">ndo_rx_flow_steer</code>函数，那就是支持RFS。</p>

<h3 id="调优-启用arfs">调优: 启用aRFS</h3>

<p>假如你的网卡支持aRFS，你可以开启它并做如下配置：</p>

<ul>
  <li>打开并配置RPS</li>
  <li>打开并配置RFS</li>
  <li>内核中编译期间指定了<code class="highlighter-rouge">CONFIG_RFS_ACCEL</code>选项。Ubuntu kernel 3.13.0是有的</li>
  <li>打开网卡的ntuple支持。可以用ethtool查看当前的ntuple设置</li>
  <li>配置IRQ（硬中断）中每个RX和CPU的对应关系</li>
</ul>

<p>以上配置完成后，aRFS就会自动将RX queue数据移动到指定CPU的内存，每个flow的包都会
到达同一个CPU，不需要你再通过ntuple手动指定每个flow的配置了。</p>


  <!-- POST NAVIGATION -->
  <div class="postNav clearfix">
     
      <a class="prev" href="/blog/tuning-stack-rx-zh-1/"><span>&laquo;&nbsp;[译] Linux网络栈监控和调优：接收数据 1</span>
      
    </a>
      
      
      <a class="next" href="/blog/tuning-stack-rx-zh-3/"><span>[译] Linux网络栈监控和调优：接收数据 3&nbsp;&raquo;</span>
       
      </a>
     
  </div>
</div>


         

      </div>
   </div><!-- end .content -->

   <div class="footer">
   <div class="container">
      <p class="copy">&copy; 2016-2019
      <a href="https://arthurchiao.github.io">Arthur Chiao</a>, Powered by
      <a href="http://jekyllrb.com">Jekyll </a>, Theme originated from
      <a href="https://github.com/brianmaierjr/long-haul"> Long Haul.</a>

      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_site_pv"> Site visits:
          <span id="busuanzi_value_site_pv"></span>, powered by<a href="http://busuanzi.ibruce.info/"> busuanzi</a>
      </span>

      </p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/profile.php?id=100014334455077">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://linkedin.com/in/yanan-zhao-7691317b?trk=hp-identity-photo">
                  <svg id="linkedin" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/ArthurChiao">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="mailto:arthurchiao@hotmail.com">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->

   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>


</body>

</html>
